// Declare table & functions
func run_sql(code) {
    print code
    force_eval( SQL( nulltype, code ))
}

// Cleanup
func cleanup_double_quote() {
    run_sql("DROP TABLE IF EXISTS \"Rating\"")
    run_sql("DROP TABLE IF EXISTS \"tmp_rating\"")
    run_sql("DROP TABLE IF EXISTS \"Rating_del1\"")
    run_sql("DROP TABLE IF EXISTS \"Rating_update1\"")
    run_sql("DROP TABLE IF EXISTS \"Rating_update001p\"")
    run_sql("DROP TABLE IF EXISTS \"Rating_update1p\"")
    run_sql("DROP TABLE IF EXISTS \"Rating_del1p\"")
    run_sql("DROP TABLE IF EXISTS \"Rating_update50p\"")
    commit()
}

func cleanup_backtick() {
    run_sql("DROP TABLE IF EXISTS `Rating`")
    run_sql("DROP TABLE IF EXISTS `tmp_rating`")
    run_sql("DROP TABLE IF EXISTS `Rating_del1`")
    run_sql("DROP TABLE IF EXISTS `Rating_update1`")
    run_sql("DROP TABLE IF EXISTS `Rating_update001p`")
    run_sql("DROP TABLE IF EXISTS `Rating_update1p`")
    run_sql("DROP TABLE IF EXISTS `Rating_del1p`")
    run_sql("DROP TABLE IF EXISTS `Rating_update50p`")
    commit()
}

if (db_type == "mysql") {
    cleanup_backtick()
} else {
    cleanup_double_quote()
}

// Import CSV
if (db_type == "snowflake") {
    print "Uploading ratings CSV"

    run_sql("RM @~/ratings.csv.gz")
    run_sql("PUT file://ml-25m/ratings.csv @~")

    print "Loading ratings CSV"

    table tmp_rating {
        id: int
        movieId: int
        rating: float
        timestamp: int
    }

    run_sql("COPY INTO tmp_rating FROM '@~/ratings.csv.gz' file_format=(skip_header=1)")

    table Rating {
        id: int
        userId: int
        movieId: int
        rating: float
        timestamp: int
    }

    run_sql("""
        INSERT INTO rating(id, userid, movieid, rating, timestamp)
        SELECT row_number() over (order by tmp_rating.id, movieid, timestamp) AS id, tmp_rating.id as userid, movieid, rating, timestamp FROM tmp_rating
    """)

} else if (db_type == "redshift") {
    // NOTE: Requires that the csv already exists on s3 in the given path
    print "Loading ratings CSV (already uploaded)"

    table tmp_rating {
        userid: int
        movieid: int
        rating: float
        timestamp: int
    }

    run_sql("""
        COPY "public"."tmp_rating" (userid, movieid, rating, timestamp)
        FROM 's3://dev-cf-redshift-datafold-xdiff/ml-25m/ratings.csv' 
        IAM_ROLE 'arn:aws:iam::760878568205:role/dev-cf-redshift-xdiff'
        DELIMITER ','
        IGNOREHEADER 1;
        """)

    table Rating {
        id: int     // explicit id, to avoid identity type
        userid: int
        movieid: int
        rating: float
        timestamp: int
    }

    run_sql("""
        INSERT INTO rating(id, userid, movieid, rating, timestamp)
        SELECT row_number() over (order by userid, movieid, timestamp) AS id, userid, movieid, rating, timestamp FROM tmp_rating
    """)
} else {
    print "Importing ratings CSV"

    table Rating {
        userId: int
        movieId: int
        rating: float
        timestamp: int
    }
    import_csv(Rating, 'ml-25m/ratings.csv', true)
    Rating.add_index("id")
}

run_sql("DROP TABLE IF EXISTS tmp_rating")
commit()

middle = count(Rating) /~ 2

// Code notes:
// - We use 'const table' to avoid updating the ids

// Rating_del1 = Delete middle row
print "Create Rating_del1"
const table Rating_del1 = Rating
Rating_del1.add_index("id")
Rating_del1[middle..(middle+1)] delete [true]
assert count(Rating) == count(Rating_del1) + 1

// Rating_update1 = Update middle row
print "Create Rating_update1"
const table Rating_update1 = Rating
Rating_update1.add_index("id")
Rating_update1[middle..(middle+1)] update {timestamp: timestamp + 1}

// Rating_<>p = Percentile of rows changed
print "Create percentile tables"
const table Rating_update001p = Rating
const table Rating_update1p = Rating
const table Rating_del1p = Rating
const table Rating_update50p = Rating

Rating_update001p.add_index("id")
Rating_update1p.add_index("id")
Rating_del1p.add_index("id")
Rating_update50p.add_index("id")

if (db_type == "postgres" or db_type == "redshift") {
    run_sql('UPDATE "Rating_update001p" SET "timestamp" = ("timestamp" + 1) WHERE random() < 0.0001')
    run_sql('UPDATE "Rating_update1p" SET "timestamp" = ("timestamp" + 1) WHERE random() < 0.01')
    run_sql('DELETE FROM "Rating_del1p" WHERE random() < 0.01')
    run_sql('UPDATE "Rating_update50p" SET "timestamp" = ("timestamp" + 1) WHERE random() < 0.5')
} else if (db_type == "mysql") {
    run_sql('UPDATE Rating_update001p SET timestamp = (timestamp + 1) WHERE rand() < 0.0001')
    run_sql('UPDATE Rating_update1p SET timestamp = (timestamp + 1) WHERE rand() < 0.01')
    run_sql('DELETE FROM Rating_del1p WHERE rand() < 0.01')
    run_sql('UPDATE Rating_update50p SET timestamp = (timestamp + 1) WHERE rand() < 0.5')
} else if (db_type == "snowflake") {
    run_sql('UPDATE Rating_update001p SET timestamp = (timestamp + 1) WHERE uniform(0::float, 1, random()) < 0.0001')
    run_sql('UPDATE Rating_update1p SET timestamp = (timestamp + 1) WHERE uniform(0::float, 1, random()) < 0.01')
    run_sql('DELETE FROM Rating_del1p WHERE uniform(0::float, 1, random()) < 0.01')
    run_sql('UPDATE Rating_update50p SET timestamp = (timestamp + 1) WHERE uniform(0::float, 1, random()) < 0.5')
} else {
    print "Unsupported database: " + db_type
}

commit()