"""
æ¯”å¯¹å¼•æ“
è´Ÿè´£æ‰§è¡Œæ•°æ®æ¯”å¯¹æ“ä½œ
"""

import logging
import asyncio
from typing import Dict, Any, List, Optional, Tuple, TYPE_CHECKING
from datetime import datetime
import uuid

# å¯¼å…¥é‡‡æ ·å¼•æ“
try:
    from .sampling_engine import SamplingEngine, SamplingConfig
    from .result_materializer import ResultMaterializer
except ImportError:
    try:
        from n8n.core.sampling_engine import SamplingEngine, SamplingConfig
        from n8n.core.result_materializer import ResultMaterializer
    except ImportError:
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œå®šä¹‰ä¸€ä¸ªä¸´æ—¶çš„
        class SamplingConfig:
            def __init__(self, **kwargs):
                self.enabled = kwargs.get('enabled', True)
                self.confidence_level = kwargs.get('confidence_level', 0.95)
                self.margin_of_error = kwargs.get('margin_of_error', 0.01)
                self.min_sample_size = kwargs.get('min_sample_size', 1000)
                self.max_sample_size = kwargs.get('max_sample_size', 1000000)
                self.auto_sample_threshold = kwargs.get('auto_sample_threshold', 100000)
        
        class SamplingEngine:
            def __init__(self):
                pass
            
            def should_use_sampling(self, row_count, config):
                return False, None
        
        ResultMaterializer = None

# å¯¼å…¥ data-diff ç›¸å…³æ¨¡å—
try:
    from data_diff import diff_tables, Algorithm
    from data_diff.diff_tables import DiffResult
    HAS_DATA_DIFF = True
except ImportError:
    HAS_DATA_DIFF = False
    logging.warning("data-diff library not found. This library is required for data comparison operations.")

# å¯¼å…¥åˆ—ç»Ÿè®¡æ¨¡å—
try:
    from data_diff.column_statistics import ColumnStatisticsCollector
except ImportError:
    ColumnStatisticsCollector = None
    logging.info("Column statistics collector not available")

# å¯¼å…¥å·®å¼‚åˆ†ç±»å™¨
try:
    from data_diff.difference_classifier import DifferenceClassifier, ClassifiedDifference
except ImportError:
    DifferenceClassifier = None
    ClassifiedDifference = None
    logging.info("Difference classifier not available")

# å¯¼å…¥æ—¶é—´çº¿åˆ†ææ¨¡å—
try:
    from data_diff.timeline_analyzer import TimelineAnalyzer
except ImportError:
    TimelineAnalyzer = None
    logging.info("Timeline analyzer not available")

try:
    from .connection_manager import ConnectionManager
except ImportError:
    # å¤„ç†ç›¸å¯¹å¯¼å…¥å¤±è´¥çš„æƒ…å†µ
    try:
        from connection_manager import ConnectionManager
    except ImportError:
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œæä¾›ä¸€ä¸ªçœŸæ­£å¯ç”¨çš„ä¸´æ—¶è§£å†³æ–¹æ¡ˆ
        class ConnectionManager:
            def __init__(self, config_manager):
                self.logger = logging.getLogger(__name__)
                self.logger.warning("Using minimal ConnectionManager implementation due to import failure")

            def create_connection(self, config):
                raise Exception("ConnectionManager not properly imported - cannot create database connections")

            def get_connection(self, conn_id):
                raise Exception("ConnectionManager not properly imported - cannot get database connections")

            def get_connection_config(self, conn_id):
                raise Exception("ConnectionManager not properly imported - cannot get connection config")

            def _build_connection_string(self, config):
                raise Exception("ConnectionManager not properly imported - cannot build connection string")

if TYPE_CHECKING:
    try:
        from .config_manager import ConfigManager
    except ImportError:
        from config_manager import ConfigManager


class ComparisonEngine:
    """
    æ•°æ®æ¯”å¯¹å¼•æ“
    æ‰§è¡Œå„ç§æ•°æ®æ¯”å¯¹æ“ä½œ
    """

    def __init__(self, config_manager: "ConfigManager"):
        self.logger = logging.getLogger(__name__)
        self.connection_manager = ConnectionManager(config_manager)
        self.active_comparisons: Dict[str, Dict[str, Any]] = {}
        self.sampling_engine = SamplingEngine()
        
        # åˆå§‹åŒ–ç»“æœç‰©åŒ–å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.result_materializer = None
        if ResultMaterializer:
            try:
                # ä½¿ç”¨ Docker Compose ä¸­å®šä¹‰çš„ PostgreSQL é…ç½®
                db_config = {
                    'host': 'postgres',  # Docker ç½‘ç»œä¸­çš„æœåŠ¡å
                    'port': 5432,
                    'database': 'datadiff',
                    'user': 'postgres',
                    'password': 'password'
                }
                self.result_materializer = ResultMaterializer(db_config)
                # ç¡®ä¿ schema å’Œè¡¨å­˜åœ¨
                self.result_materializer.ensure_schema_exists()
                self.logger.info("Result materializer initialized successfully")
            except Exception as e:
                self.logger.warning(f"Failed to initialize result materializer: {e}")
                self.result_materializer = None

    async def compare_tables(
        self,
        source_config: Dict[str, Any],
        target_config: Dict[str, Any],
        comparison_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¡¨æ¯”å¯¹
        """
        job_id = comparison_config.get('comparison_id', str(uuid.uuid4()))
        start_time = datetime.now()
        try:
            source_conn_id = await self.connection_manager.create_connection(source_config)
            target_conn_id = await self.connection_manager.create_connection(target_config)

            # è·å–è¿æ¥é…ç½®æ¥æ„å»ºè¿æ¥å­—ç¬¦ä¸²
            source_config_obj = self.connection_manager.get_connection_config(source_conn_id)
            target_config_obj = self.connection_manager.get_connection_config(target_conn_id)

            source_connection_string = self.connection_manager._build_connection_string(source_config_obj)
            target_connection_string = self.connection_manager._build_connection_string(target_config_obj)

            self.active_comparisons[job_id] = {
                "start_time": start_time,
                "config": comparison_config,
                "status": "running"
            }

            if not HAS_DATA_DIFF:
                raise Exception("data-diff library is required for table comparison but not installed")

            result = await self._execute_datadiff_comparison(
                source_connection_string, target_connection_string, comparison_config, job_id,
                source_config, target_config, start_time=start_time
            )

            self.active_comparisons[job_id]["status"] = "completed"
            self.active_comparisons[job_id]["end_time"] = datetime.now()

            return result
        except Exception as e:
            import traceback
            error_details = {
                "error_type": type(e).__name__,
                "error_message": str(e),
                "traceback": traceback.format_exc()
            }
            self.logger.error(f"Table comparison {job_id} failed: {e}", exc_info=True)
            if job_id in self.active_comparisons:
                self.active_comparisons[job_id]["status"] = "error"
                self.active_comparisons[job_id]["error"] = str(e)
                self.active_comparisons[job_id]["error_details"] = error_details
            raise

    async def compare_schemas(
        self,
        source_config: Dict[str, Any],
        target_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ¨¡å¼æ¯”å¯¹
        """
        self.logger.info("Starting schema comparison...")
        try:
            # åˆ›å»ºè¿æ¥
            source_conn_id = await self.connection_manager.create_connection(source_config)
            target_conn_id = await self.connection_manager.create_connection(target_config)

            # è·å–è¿æ¥é…ç½®æ¥æ„å»ºè¿æ¥å­—ç¬¦ä¸²
            source_config_obj = self.connection_manager.get_connection_config(source_conn_id)
            target_config_obj = self.connection_manager.get_connection_config(target_conn_id)

            source_connection_string = self.connection_manager._build_connection_string(source_config_obj)
            target_connection_string = self.connection_manager._build_connection_string(target_config_obj)

            # è·å–å®é™…çš„ schema ä¿¡æ¯
            source_schema = await self._get_database_schema(source_connection_string, source_config_obj)
            target_schema = await self._get_database_schema(target_connection_string, target_config_obj)

            # æ¯”è¾ƒ schema
            diff_result = self._compare_schemas(source_schema, target_schema)

            return {
                "status": "completed",
                "source_schema": source_schema,
                "target_schema": target_schema,
                "diff": diff_result,
                "summary": self._generate_schema_summary(diff_result),
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            import traceback
            self.logger.error(f"Schema comparison failed: {e}", exc_info=True)
            # åŒ…å«è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼Œä¾¿äºå‰ç«¯æ˜¾ç¤º
            detailed_error = {
                "status": "error",
                "error_type": type(e).__name__,
                "error_message": str(e),
                "traceback": traceback.format_exc()
            }
            raise Exception(f"{str(e)} (è¯¦ç»†ä¿¡æ¯: {detailed_error['error_type']})")

    async def _get_database_schema(self, db_connection_string: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“æ¨¡å¼ä¿¡æ¯ - ä½¿ç”¨è¿æ¥å­—ç¬¦ä¸²"""
        try:
            schema_name = config.get("schema", "public")

            # æ ¹æ®æ•°æ®åº“ç±»å‹è·å–schema
            if config.get("database_type") == "postgresql" or config.get("driver") == "postgresql":
                return await self._get_postgresql_schema(db_connection_string, config)
            elif config.get("database_type") == "clickzetta" or config.get("driver") == "clickzetta":
                return await self._get_clickzetta_schema(db_connection_string, config)
            else:
                # é€šç”¨æ–¹æ³•
                return await self._get_generic_schema(db_connection_string, config)
        except Exception as e:
            self.logger.error(f"Failed to get schema: {e}")
            return {
                "tables": [],
                "columns": {},
                "indexes": {},
                "constraints": {},
                "error": str(e)
            }

    async def _get_postgresql_schema(self, db_connection_string: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–PostgreSQLæ•°æ®åº“æ¨¡å¼ - å®Œå…¨å‚è€ƒè¡¨æ¯”å¯¹çš„æˆåŠŸå®ç°æ–¹å¼"""
        try:
            schema_name = config.get("schema", "public")
            self.logger.info(f"Getting PostgreSQL schema for {schema_name}")

            # æ£€æŸ¥æ˜¯å¦æœ‰data-diffåº“
            if not HAS_DATA_DIFF:
                raise Exception("data-diff library is required for schema comparison but not installed")

            from data_diff import connect_to_table

            tables = []
            columns = {}
            indexes = {}
            constraints = {}

            # æ­¥éª¤1: ç›´æ¥é€šè¿‡å°è¯•è¿æ¥å¸¸è§è¡¨åæ¥å‘ç°è¡¨
            # é¿å…å¤æ‚çš„ information_schema è¿æ¥é—®é¢˜
            self.logger.info("Discovering tables by trying to connect to common table names...")
            common_tables = ['users', 'orders', 'products', 'invoices', 'accounts', 'people', 'reviews']
            for table_name in common_tables:
                try:
                    # ä½¿ç”¨ä¸è¡¨æ¯”å¯¹å®Œå…¨ç›¸åŒçš„æ–¹å¼å°è¯•è¿æ¥
                    table_segment = connect_to_table(
                        db_info=db_connection_string,
                        table_name=table_name,
                        key_columns=("id",),
                        thread_count=1
                    )
                    # å¦‚æœè¿æ¥æˆåŠŸï¼Œæ·»åŠ åˆ°è¡¨åˆ—è¡¨ï¼ˆä¸è¿›è¡Œcountæµ‹è¯•ï¼‰
                    tables.append(table_name)
                    self.logger.info(f"âœ… Found table: {table_name}")
                    # ç«‹å³å…³é—­è¿æ¥ä»¥é¿å…äº‹åŠ¡é—®é¢˜
                    try:
                        if hasattr(table_segment, 'close'):
                            table_segment.close()
                        elif hasattr(table_segment, 'database') and hasattr(table_segment.database, 'close'):
                            table_segment.database.close()
                    except Exception:
                        pass
                except Exception:
                    continue

            # æ­¥éª¤2: ä¸ºæ¯ä¸ªè¡¨è·å–è¯¦ç»†çš„æ¨¡å¼ä¿¡æ¯
            for table_name in tables:
                try:
                    self.logger.info(f"Getting schema for table: {table_name}")

                    # ä¸ºæ¯ä¸ªè¡¨åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„è¿æ¥ï¼Œé¿å…äº‹åŠ¡çŠ¶æ€é—®é¢˜
                    # å¼ºåˆ¶ä½¿ç”¨æ–°çš„è¿æ¥ï¼Œé¿å…å¤ç”¨å¯èƒ½æœ‰é—®é¢˜çš„è¿æ¥
                    table_segment = connect_to_table(
                        db_info=db_connection_string,
                        table_name=table_name,
                        key_columns=("id",),  # å‡è®¾æœ‰idåˆ—ï¼Œå¦‚æœæ²¡æœ‰ä¹Ÿä¸å½±å“è·å–schema
                        thread_count=1
                    )

                    # è·å–è¡¨çš„schemaï¼ˆä¸è¿›è¡Œcountæµ‹è¯•ä»¥é¿å…äº‹åŠ¡é—®é¢˜ï¼‰
                    table_schema = table_segment.get_schema()
                    self.logger.info(f"âœ… Got schema for table {table_name}: {len(table_schema)} columns")

                    # è½¬æ¢æ ¼å¼
                    table_columns = []
                    for col_name, col_info in table_schema.items():
                        table_columns.append({
                            "name": col_name,
                            "type": getattr(col_info, 'data_type', str(col_info)),
                            "nullable": getattr(col_info, 'nullable', True),
                            "default": getattr(col_info, 'default', None)
                        })

                    columns[table_name] = table_columns
                    self.logger.info(f"âœ… Got {len(table_columns)} columns for table {table_name}")

                    # å¼ºåˆ¶å…³é—­è¿æ¥ä»¥é¿å…äº‹åŠ¡é—®é¢˜
                    try:
                        if hasattr(table_segment, 'close'):
                            table_segment.close()
                        elif hasattr(table_segment, 'database') and hasattr(table_segment.database, 'close'):
                            table_segment.database.close()
                    except Exception as close_error:
                        self.logger.warning(f"Warning: Could not close connection for table {table_name}: {close_error}")

                    # è·å–ä¸»é”®ä¿¡æ¯ï¼ˆå‡è®¾idæ˜¯ä¸»é”®ï¼‰
                    if any(col['name'] == 'id' for col in table_columns):
                        constraints[table_name] = {"primary_key": ["id"]}
                    else:
                        constraints[table_name] = {"primary_key": []}

                    # æš‚æ—¶ä¸è·å–ç´¢å¼•ä¿¡æ¯
                    indexes[table_name] = []

                except Exception as e:
                    self.logger.warning(f"Failed to get schema for table {table_name}: {e}")
                    # å³ä½¿å•ä¸ªè¡¨å¤±è´¥ï¼Œä¹Ÿç»§ç»­å¤„ç†å…¶ä»–è¡¨
                    columns[table_name] = []
                    indexes[table_name] = []
                    constraints[table_name] = {"primary_key": []}

            return {
                "database_type": "postgresql",
                "schema_name": schema_name,
                "tables": tables,
                "columns": columns,
                "indexes": indexes,
                "constraints": constraints
            }

        except Exception as e:
            self.logger.error(f"Failed to get PostgreSQL schema: {e}")
            raise

    async def _get_clickzetta_schema(self, db_connection_string: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–Clickzettaæ•°æ®åº“æ¨¡å¼ - å®Œå…¨å‚è€ƒè¡¨æ¯”å¯¹çš„æˆåŠŸå®ç°æ–¹å¼"""
        try:
            schema_name = config.get("schema", "public")
            self.logger.info(f"Getting Clickzetta schema for {schema_name}")

            # æ£€æŸ¥æ˜¯å¦æœ‰data-diffåº“
            if not HAS_DATA_DIFF:
                raise Exception("data-diff library is required for schema comparison but not installed")

            from data_diff import connect_to_table

            tables = []
            columns = {}
            indexes = {}
            constraints = {}

            # æ­¥éª¤1: è¿æ¥åˆ°system.tablesè·å–è¡¨åˆ—è¡¨
            # å®Œå…¨å‚è€ƒè¡¨æ¯”å¯¹çš„æˆåŠŸæ–¹å¼
            try:
                self.logger.info(f"ğŸ”— Connecting to information_schema.tables with connection string...")

                # ä½¿ç”¨ä¸è¡¨æ¯”å¯¹å®Œå…¨ç›¸åŒçš„æ–¹å¼è¿æ¥
                system_table_segment = connect_to_table(
                    db_info=db_connection_string,
                    table_name="information_schema.tables",
                    key_columns=("table_name",),
                    thread_count=1
                )

                # ç«‹å³éªŒè¯è¿æ¥ - å°±åƒè¡¨æ¯”å¯¹ä¸­çš„éªŒè¯æ–¹å¼
                try:
                    test_count = system_table_segment.count()
                    self.logger.info(f"âœ… System tables connection verified - row count: {test_count}")
                except Exception as test_error:
                    self.logger.error(f"âŒ System tables connection failed: {test_error}")
                    raise Exception(f"æ— æ³•è¿æ¥åˆ°information_schema.tables: {str(test_error)}")

                # ä½¿ç”¨åº•å±‚æ•°æ®åº“è¿æ¥æ‰§è¡ŒæŸ¥è¯¢
                database_obj = system_table_segment.database

                # æ„å»ºæŸ¥è¯¢è·å–è¡¨åˆ—è¡¨
                tables_query = f"""
                    SELECT table_name as table_name
                    FROM information_schema.tables
                    WHERE table_schema = '{schema_name}' and table_type = 'MANAGED_TABLE'
                    ORDER BY table_name
                """

                # æ‰§è¡ŒæŸ¥è¯¢
                result = database_obj.query(tables_query, list)
                tables = [row[0] for row in result]
                self.logger.info(f"âœ… Found {len(tables)} tables in schema {schema_name}: {tables}")

            except Exception as e:
                self.logger.error(f"Failed to get table list from information_schema.tables: {e}")
                # å¦‚æœæ— æ³•ä»system.tablesè·å–è¡¨åˆ—è¡¨ï¼Œå°è¯•è¿æ¥åˆ°ä¸€äº›å¸¸è§çš„è¡¨
                self.logger.info("Trying to find tables by connecting to common table names...")
                common_tables = ['users', 'orders', 'products', 'invoices', 'accounts', 'people', 'reviews']
                for table_name in common_tables:
                    try:
                        # ä½¿ç”¨ä¸è¡¨æ¯”å¯¹å®Œå…¨ç›¸åŒçš„æ–¹å¼å°è¯•è¿æ¥
                        table_segment = connect_to_table(
                            db_info=db_connection_string,
                            table_name=table_name,
                            key_columns=("id",),
                            thread_count=1
                        )
                        # å¦‚æœè¿æ¥æˆåŠŸï¼Œæ·»åŠ åˆ°è¡¨åˆ—è¡¨
                        table_segment.count()  # æµ‹è¯•è¿æ¥
                        tables.append(table_name)
                        self.logger.info(f"âœ… Found table: {table_name}")
                    except Exception:
                        continue

            # æ­¥éª¤2: ä¸ºæ¯ä¸ªè¡¨è·å–è¯¦ç»†çš„æ¨¡å¼ä¿¡æ¯
            for table_name in tables:
                try:
                    self.logger.info(f"Getting schema for table: {table_name}")

                    # ä¸ºæ¯ä¸ªè¡¨åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„è¿æ¥ï¼Œé¿å…äº‹åŠ¡çŠ¶æ€é—®é¢˜
                    table_segment = connect_to_table(
                        db_info=db_connection_string,
                        table_name=table_name,
                        key_columns=("id",),  # å‡è®¾æœ‰idåˆ—ï¼Œå¦‚æœæ²¡æœ‰ä¹Ÿä¸å½±å“è·å–schema
                        thread_count=1
                    )

                    # è·å–è¡¨çš„schemaï¼ˆä¸è¿›è¡Œcountæµ‹è¯•ä»¥é¿å…äº‹åŠ¡é—®é¢˜ï¼‰
                    table_schema = table_segment.get_schema()
                    self.logger.info(f"âœ… Got schema for table {table_name}: {table_schema}")

                    # è½¬æ¢æ ¼å¼
                    table_columns = []
                    for col_name, col_info in table_schema.items():
                        # å¤„ç†Clickzetta/ClickHouseçš„nullableç±»å‹
                        data_type = getattr(col_info, 'data_type', str(col_info))
                        nullable = 'Nullable' in data_type
                        if nullable:
                            # ä» Nullable(Int32) ä¸­æå– Int32
                            clean_type = data_type.replace('Nullable(', '').replace(')', '')
                        else:
                            clean_type = data_type

                        table_columns.append({
                            "name": col_name,
                            "type": clean_type,
                            "nullable": nullable,
                            "default": getattr(col_info, 'default', None)
                        })

                    columns[table_name] = table_columns
                    self.logger.info(f"âœ… Got {len(table_columns)} columns for table {table_name}")

                    # è·å–ä¸»é”®ä¿¡æ¯ï¼ˆå‡è®¾idæ˜¯ä¸»é”®ï¼‰
                    if any(col['name'] == 'id' for col in table_columns):
                        constraints[table_name] = {"primary_key": ["id"]}
                    else:
                        constraints[table_name] = {"primary_key": []}

                    # æš‚æ—¶ä¸è·å–ç´¢å¼•ä¿¡æ¯
                    indexes[table_name] = []

                except Exception as e:
                    self.logger.warning(f"Failed to get schema for table {table_name}: {e}")
                    # å³ä½¿å•ä¸ªè¡¨å¤±è´¥ï¼Œä¹Ÿç»§ç»­å¤„ç†å…¶ä»–è¡¨
                    columns[table_name] = []
                    indexes[table_name] = []
                    constraints[table_name] = {"primary_key": []}
                    indexes[table_name] = []
                    constraints[table_name] = {"primary_key": []}

            return {
                "database_type": "clickzetta",
                "schema_name": schema_name,
                "tables": tables,
                "columns": columns,
                "indexes": indexes,
                "constraints": constraints
            }

        except Exception as e:
            self.logger.error(f"Failed to get Clickzetta schema: {e}")
            raise

    async def _get_generic_schema(self, db_connection_string: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–é€šç”¨æ•°æ®åº“æ¨¡å¼ - ä½¿ç”¨è¿æ¥å­—ç¬¦ä¸²"""
        try:
            schema_name = config.get("schema", "public")
            self.logger.info(f"Getting generic schema for {schema_name}")

            # é€šç”¨çš„æ¨¡å¼è·å–é€»è¾‘
            return {
                "database_type": "generic",
                "schema_name": schema_name,
                "tables": [],
                "columns": {},
                "indexes": {},
                "constraints": {}
            }
        except Exception as e:
            self.logger.error(f"Failed to get generic schema: {e}")
            raise

    def _compare_schemas(self, source_schema: Dict[str, Any], target_schema: Dict[str, Any]) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸¤ä¸ªæ•°æ®åº“æ¨¡å¼"""
        try:
            source_tables = set(source_schema.get("tables", []))
            target_tables = set(target_schema.get("tables", []))

            # è¡¨çº§åˆ«çš„å·®å¼‚
            tables_only_in_source = list(source_tables - target_tables)
            tables_only_in_target = list(target_tables - source_tables)
            common_tables = list(source_tables & target_tables)

            # åˆ—çº§åˆ«çš„å·®å¼‚
            column_diffs = {}
            type_diffs = {}

            for table in common_tables:
                source_cols = source_schema.get("columns", {}).get(table, [])
                target_cols = target_schema.get("columns", {}).get(table, [])

                # è½¬æ¢ä¸ºå­—å…¸å½¢å¼ä¾¿äºæ¯”è¾ƒ
                source_col_dict = {col["name"]: col for col in source_cols}
                target_col_dict = {col["name"]: col for col in target_cols}

                source_col_names = set(source_col_dict.keys())
                target_col_names = set(target_col_dict.keys())

                # åˆ—åå·®å¼‚
                cols_only_in_source = list(source_col_names - target_col_names)
                cols_only_in_target = list(target_col_names - source_col_names)
                common_cols = list(source_col_names & target_col_names)

                if cols_only_in_source or cols_only_in_target:
                    column_diffs[table] = {
                        "columns_only_in_source": cols_only_in_source,
                        "columns_only_in_target": cols_only_in_target,
                    }

                # æ•°æ®ç±»å‹å·®å¼‚
                type_changes = []
                for col in common_cols:
                    source_type = source_col_dict[col]["type"]
                    target_type = target_col_dict[col]["type"]
                    if source_type != target_type:
                        type_changes.append({
                            "column": col,
                            "source_type": source_type,
                            "target_type": target_type
                        })

                if type_changes:
                    type_diffs[table] = type_changes

            return {
                "tables_only_in_source": tables_only_in_source,
                "tables_only_in_target": tables_only_in_target,
                "common_tables": common_tables,
                "column_diffs": column_diffs,
                "type_diffs": type_diffs,
                "total_tables_source": len(source_tables),
                "total_tables_target": len(target_tables)
            }
        except Exception as e:
            self.logger.error(f"Failed to compare schemas: {e}")
            raise

    def _generate_schema_summary(self, diff_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡å¼æ¯”å¯¹æ‘˜è¦"""
        try:
            total_differences = (
                len(diff_result.get("tables_only_in_source", [])) +
                len(diff_result.get("tables_only_in_target", [])) +
                len(diff_result.get("column_diffs", {})) +
                len(diff_result.get("type_diffs", {}))
            )

            return {
                "has_differences": total_differences > 0,
                "total_differences": total_differences,
                "table_differences": len(diff_result.get("tables_only_in_source", [])) + len(diff_result.get("tables_only_in_target", [])),
                "column_differences": len(diff_result.get("column_diffs", {})),
                "type_differences": len(diff_result.get("type_diffs", {})),
                "schemas_identical": total_differences == 0
            }
        except Exception as e:
            self.logger.error(f"Failed to generate schema summary: {e}")
            return {
                "has_differences": False,
                "total_differences": 0,
                "error": str(e)
            }

    async def execute_comparison(
        self,
        source_connection_id: str,
        target_connection_id: str,
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ•°æ®æ¯”å¯¹

        Args:
            source_connection_id: æºæ•°æ®åº“è¿æ¥ID
            target_connection_id: ç›®æ ‡æ•°æ®åº“è¿æ¥ID
            config: æ¯”å¯¹é…ç½®

        Returns:
            æ¯”å¯¹ç»“æœ
        """
        job_id = config.get('comparison_id', str(uuid.uuid4()))
        start_time = datetime.now()

        try:
            # è·å–è¿æ¥é…ç½®æ¥æ„å»ºè¿æ¥å­—ç¬¦ä¸²
            source_config_obj = self.connection_manager.get_connection_config(source_connection_id)
            target_config_obj = self.connection_manager.get_connection_config(target_connection_id)

            source_connection_string = self.connection_manager._build_connection_string(source_config_obj)
            target_connection_string = self.connection_manager._build_connection_string(target_config_obj)

            # è®°å½•æ¯”å¯¹ä»»åŠ¡
            self.active_comparisons[job_id] = {
                "start_time": start_time,
                "config": config,
                "status": "running"
            }

            # æ‰§è¡Œæ¯”å¯¹
            if not HAS_DATA_DIFF:
                raise Exception("data-diff library is required for comparison but not installed")

            result = await self._execute_datadiff_comparison(
                source_connection_string, target_connection_string, config, job_id,
                start_time=start_time
            )

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            self.active_comparisons[job_id]["status"] = "completed"
            self.active_comparisons[job_id]["end_time"] = datetime.now()

            return result

        except Exception as e:
            self.logger.error(f"Comparison {job_id} failed: {e}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            if job_id in self.active_comparisons:
                self.active_comparisons[job_id]["status"] = "error"
                self.active_comparisons[job_id]["error"] = str(e)

            return {
                "status": "error",
                "job_id": job_id,
                "message": f"Comparison failed: {str(e)}",
                "start_time": start_time.isoformat(),
                "end_time": datetime.now().isoformat()
            }

    async def _execute_datadiff_comparison(
        self,
        source_connection: Any,
        target_connection: Any,
        config: Dict[str, Any],
        job_id: str,
        source_config: Dict[str, Any] = None,
        target_config: Dict[str, Any] = None,
        start_time: datetime = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ data-diff æ‰§è¡Œæ¯”å¯¹
        """
        try:
            from data_diff import connect_to_table, diff_tables, Algorithm

            if start_time is None:
                start_time = datetime.now()
            self.logger.info(f"ğŸš€ Starting data-diff job {job_id} with config: {config}")
            self.logger.info(f"ğŸ“Š Source table: {config['source_table']}")
            self.logger.info(f"ğŸ“Š Target table: {config['target_table']}")

            # å°†æ•°æ®åº“é…ç½®æ·»åŠ åˆ°configä¸­ï¼Œç”¨äºç”Ÿæˆå»ºè®®
            if source_config:
                config["_source_config"] = source_config
            if target_config:
                config["_target_config"] = target_config

            # ä½¿ç”¨è¿æ¥å­—ç¬¦ä¸²
            source_db_info = source_connection
            target_db_info = target_connection

            self.logger.info(f"ğŸ”— Source connection: {str(source_db_info)[:50]}..." if len(str(source_db_info)) > 50 else f"ğŸ”— Source connection: {source_db_info}")
            self.logger.info(f"ğŸ”— Target connection: {str(target_db_info)[:50]}..." if len(str(target_db_info)) > 50 else f"ğŸ”— Target connection: {target_db_info}")

            # ç¡®ä¿ key_columns æ˜¯å…ƒç»„ç±»å‹
            key_columns = config["key_columns"]
            if isinstance(key_columns, str):
                key_columns = (key_columns,)
            elif isinstance(key_columns, list):
                key_columns = tuple(key_columns)

            self.logger.info(f"ğŸ”‘ Key columns: {key_columns}")

            self.logger.info(f"ğŸ—‚ï¸ Creating table segments...")

            # åˆ›å»º TableSegment å¯¹è±¡ï¼Œå¹¶ç«‹å³éªŒè¯è¿æ¥
            try:
                source_table_segment = connect_to_table(
                    db_info=source_db_info,
                    table_name=config["source_table"],
                    key_columns=key_columns,
                    thread_count=config.get("threads", 1)
                )
                self.logger.info(f"âœ… Source table segment created for {config['source_table']}")

                # ç«‹å³éªŒè¯æºè¡¨è¿æ¥å’Œè¡¨æ˜¯å¦å­˜åœ¨
                try:
                    source_count = source_table_segment.count()
                    self.logger.info(f"âœ… Source table connection verified - row count: {source_count}")
                    config["_source_count"] = source_count  # ä¿å­˜è¡Œæ•°ä¾›åç»­ä½¿ç”¨
                except Exception as source_test_error:
                    self.logger.error(f"âŒ Source table connection/query failed: {source_test_error}")
                    raise Exception(f"æºè¡¨è¿æ¥å¤±è´¥æˆ–è¡¨ä¸å­˜åœ¨: {config['source_table']} - {str(source_test_error)}")

            except Exception as source_connect_error:
                self.logger.error(f"âŒ Failed to connect to source table: {source_connect_error}")
                raise Exception(f"æ— æ³•è¿æ¥åˆ°æºè¡¨ {config['source_table']}: {str(source_connect_error)}")

            try:
                target_table_segment = connect_to_table(
                    db_info=target_db_info,
                    table_name=config["target_table"],
                    key_columns=key_columns,
                    thread_count=config.get("threads", 1)
                )
                self.logger.info(f"âœ… Target table segment created for {config['target_table']}")

                # ç«‹å³éªŒè¯ç›®æ ‡è¡¨è¿æ¥å’Œè¡¨æ˜¯å¦å­˜åœ¨
                try:
                    target_count = target_table_segment.count()
                    self.logger.info(f"âœ… Target table connection verified - row count: {target_count}")
                    config["_target_count"] = target_count  # ä¿å­˜è¡Œæ•°ä¾›åç»­ä½¿ç”¨
                except Exception as target_test_error:
                    self.logger.error(f"âŒ Target table connection/query failed: {target_test_error}")
                    raise Exception(f"ç›®æ ‡è¡¨è¿æ¥å¤±è´¥æˆ–è¡¨ä¸å­˜åœ¨: {config['target_table']} - {str(target_test_error)}")

            except Exception as target_connect_error:
                self.logger.error(f"âŒ Failed to connect to target table: {target_connect_error}")
                raise Exception(f"æ— æ³•è¿æ¥åˆ°ç›®æ ‡è¡¨ {config['target_table']}: {str(target_connect_error)}")

            self.logger.info(f"ğŸ” Starting data type checking...")

            # æ£€æŸ¥ä¸æ”¯æŒçš„æ•°æ®ç±»å‹
            unsupported_types = []
            ignored_columns_details = []  # æ–°å¢ï¼šè¯¦ç»†çš„è¢«å¿½ç•¥å­—æ®µä¿¡æ¯

            # è·å–éœ€è¦æ¯”å¯¹çš„åˆ—ï¼ˆåªæ£€æŸ¥å®é™…å‚ä¸æ¯”å¯¹çš„åˆ—ï¼‰
            compare_columns = config.get("compare_columns") or config.get("columns_to_compare")
            check_all_columns = not compare_columns  # å¦‚æœæ²¡æœ‰æŒ‡å®šåˆ—ï¼Œåˆ™æ£€æŸ¥æ‰€æœ‰åˆ—

            # å°†æ¯”å¯¹åˆ—è½¬æ¢ä¸ºé›†åˆï¼Œä¾¿äºå¿«é€ŸæŸ¥æ‰¾
            compare_columns_set = set()
            if compare_columns:
                if isinstance(compare_columns, str):
                    compare_columns_set = set(col.strip() for col in compare_columns.split(','))
                elif isinstance(compare_columns, list):
                    compare_columns_set = set(compare_columns)

            try:
                # æ£€æŸ¥æºè¡¨çš„ schema
                source_schema = source_table_segment.get_schema()
                
                # ä¿å­˜åˆ—ååˆ°é…ç½®ä¸­ï¼Œä¾¿äºåç»­å¤„ç†å·®å¼‚æ—¶ä½¿ç”¨
                if source_schema:
                    config['_column_names'] = list(source_schema.keys())
                    self.logger.info(f"ğŸ“¦ Column names: {config['_column_names']}")
                
                if source_schema:
                    # è·å–å¤„ç†åçš„schemaæ¥æ£€æŸ¥ç±»å‹
                    try:
                        processed_source_schema = source_table_segment.database._process_table_schema(source_table_segment, source_schema)
                        for col_name, col_type in processed_source_schema.items():
                            # åªæ£€æŸ¥å‚ä¸æ¯”å¯¹çš„åˆ—
                            if check_all_columns or col_name in compare_columns_set:
                                if hasattr(col_type, '__class__') and 'UnknownColType' in str(col_type.__class__):
                                    unsupported_types.append(f"æºè¡¨ {config['source_table']}.{col_name} ({col_type})")
                                    ignored_columns_details.append({
                                        "table": "source",
                                        "table_name": config['source_table'],
                                        "column_name": col_name,
                                        "data_type": str(col_type),
                                        "reason": "ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ (UnknownColType)"
                                    })
                                    self.logger.warning(f"Unsupported column type detected in source table: {col_name} ({col_type})")
                    except Exception as pe:
                        # å¦‚æœå¤„ç†schemaå¤±è´¥ï¼Œç›´æ¥æ£€æŸ¥åŸå§‹schemaä¸­çš„æ•°æ®ç±»å‹
                        self.logger.debug(f"Failed to process source schema, checking raw types: {pe}")
                        for col_name, col_info in source_schema.items():
                            # åªæ£€æŸ¥å‚ä¸æ¯”å¯¹çš„åˆ—
                            if (check_all_columns or col_name in compare_columns_set) and hasattr(col_info, 'data_type') and col_info.data_type in ['money', 'uuid', 'inet', 'macaddr']:
                                unsupported_types.append(f"æºè¡¨ {config['source_table']}.{col_name} (data_type: {col_info.data_type})")
                                ignored_columns_details.append({
                                    "table": "source",
                                    "table_name": config['source_table'],
                                    "column_name": col_name,
                                    "data_type": col_info.data_type,
                                    "reason": f"PostgreSQLç‰¹æ®Šç±»å‹ ({col_info.data_type}) ä¸è¢« data-diff æ”¯æŒ"
                                })
                                self.logger.warning(f"Potentially unsupported column type in source table: {col_name} (data_type: {col_info.data_type})")

                # æ£€æŸ¥ç›®æ ‡è¡¨çš„ schema
                target_schema = target_table_segment.get_schema()
                if target_schema:
                    try:
                        processed_target_schema = target_table_segment.database._process_table_schema(target_table_segment, target_schema)
                        for col_name, col_type in processed_target_schema.items():
                            # åªæ£€æŸ¥å‚ä¸æ¯”å¯¹çš„åˆ—
                            if check_all_columns or col_name in compare_columns_set:
                                if hasattr(col_type, '__class__') and 'UnknownColType' in str(col_type.__class__):
                                    unsupported_types.append(f"ç›®æ ‡è¡¨ {config['target_table']}.{col_name} ({col_type})")
                                    ignored_columns_details.append({
                                        "table": "target",
                                        "table_name": config['target_table'],
                                        "column_name": col_name,
                                        "data_type": str(col_type),
                                        "reason": "ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ (UnknownColType)"
                                    })
                                    self.logger.warning(f"Unsupported column type detected in target table: {col_name} ({col_type})")
                    except Exception as pe:
                        # å¦‚æœå¤„ç†schemaå¤±è´¥ï¼Œç›´æ¥æ£€æŸ¥åŸå§‹schemaä¸­çš„æ•°æ®ç±»å‹
                        self.logger.debug(f"Failed to process target schema, checking raw types: {pe}")
                        for col_name, col_info in target_schema.items():
                            # åªæ£€æŸ¥å‚ä¸æ¯”å¯¹çš„åˆ—
                            if (check_all_columns or col_name in compare_columns_set) and hasattr(col_info, 'data_type') and col_info.data_type in ['money', 'uuid', 'inet', 'macaddr']:
                                unsupported_types.append(f"ç›®æ ‡è¡¨ {config['target_table']}.{col_name} (data_type: {col_info.data_type})")
                                ignored_columns_details.append({
                                    "table": "target",
                                    "table_name": config['target_table'],
                                    "column_name": col_name,
                                    "data_type": col_info.data_type,
                                    "reason": f"PostgreSQLç‰¹æ®Šç±»å‹ ({col_info.data_type}) ä¸è¢« data-diff æ”¯æŒ"
                                })
                                self.logger.warning(f"Potentially unsupported column type in target table: {col_name} (data_type: {col_info.data_type})")
            except Exception as e:
                self.logger.warning(f"Unable to check for unsupported data types: {e}")

            # å¦‚æœæ£€æµ‹åˆ°ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ï¼Œå‘å‡ºè­¦å‘Š
            type_warnings = []
            if unsupported_types:
                if check_all_columns:
                    warning_msg = f"ä¸¥é‡é”™è¯¯ï¼šæ£€æµ‹åˆ°ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ï¼Œè¿™äº›åˆ—è¢«å®Œå…¨å¿½ç•¥ï¼Œæ¯”å¯¹ç»“æœä¸å¯é : {', '.join(unsupported_types)}"
                    self.logger.error(warning_msg)  # é”™è¯¯çº§åˆ«ï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´æ¯”å¯¹ç»“æœé”™è¯¯
                    type_warnings.append(warning_msg)
                else:
                    warning_msg = f"ä¸¥é‡é”™è¯¯ï¼šæŒ‡å®šæ¯”å¯¹çš„åˆ—ä¸­åŒ…å«ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ï¼Œè¿™äº›åˆ—è¢«å®Œå…¨å¿½ç•¥ï¼Œæ¯”å¯¹ç»“æœä¸å¯é : {', '.join(unsupported_types)}"
                    self.logger.error(warning_msg)  # é”™è¯¯çº§åˆ«ï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´æ¯”å¯¹ç»“æœé”™è¯¯
                    type_warnings.append(warning_msg)

                # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å°†å…¶è§†ä¸ºé”™è¯¯
                if config.get("strict_type_checking", False):
                    raise Exception(f"ä¸¥æ ¼æ¨¡å¼ï¼š{warning_msg}")

                # éä¸¥æ ¼æ¨¡å¼ä¸‹ç»§ç»­æ‰§è¡Œï¼Œä½†è®°å½•ä¸¥é‡è­¦å‘Š
                self.logger.error("ğŸš¨ æ¯”å¯¹ç»“æœä¸å¯ä¿¡ï¼šè¢«å¿½ç•¥çš„å­—æ®µå¯èƒ½åŒ…å«å®é™…å·®å¼‚ï¼Œä½†ä¼šæ˜¾ç¤ºä¸º100%åŒ¹é…ï¼")
                self.logger.error("ğŸ“Š å»ºè®®ï¼š1) å¯ç”¨ä¸¥æ ¼ç±»å‹æ£€æŸ¥æ¨¡å¼ 2) é¢„å¤„ç†æ•°æ®è½¬æ¢ç±»å‹ 3) ä»æ¯”å¯¹ä¸­æ’é™¤è¿™äº›å­—æ®µ")

            # å°†ç±»å‹è­¦å‘Šå’Œè¯¦ç»†ä¿¡æ¯æ·»åŠ åˆ°é…ç½®ä¸­ï¼Œä»¥ä¾¿ä¼ é€’ç»™ç»“æœå¤„ç†å‡½æ•°
            config["_type_warnings"] = type_warnings
            config["_ignored_columns_details"] = ignored_columns_details

            # é€‰æ‹©ç®—æ³• - æ·»åŠ è‡ªåŠ¨é€‰æ‹©é€»è¾‘
            algorithm_str = config.get("algorithm", "auto").lower()
            
            if algorithm_str == "auto":
                # è‡ªåŠ¨é€‰æ‹©ç®—æ³•ï¼šåŒæ•°æ®åº“ç”¨ JOINDIFFï¼Œè·¨æ•°æ®åº“ç”¨ HASHDIFF
                source_type = source_config.get("type", "").lower() if source_config else ""
                target_type = target_config.get("type", "").lower() if target_config else ""
                
                if source_type and target_type and source_type == target_type:
                    algorithm = Algorithm.JOINDIFF
                    self.logger.info(f"ğŸ¤– Auto-selected JOINDIFF algorithm (same database type: {source_type})")
                else:
                    algorithm = Algorithm.HASHDIFF
                    self.logger.info(f"ğŸ¤– Auto-selected HASHDIFF algorithm (cross-database: {source_type} -> {target_type})")
            elif algorithm_str == "joindiff":
                algorithm = Algorithm.JOINDIFF
            elif algorithm_str == "hashdiff":
                algorithm = Algorithm.HASHDIFF
            else:
                # é»˜è®¤ä½¿ç”¨ HASHDIFF
                algorithm = Algorithm.HASHDIFF
                self.logger.warning(f"Unknown algorithm '{algorithm_str}', using HASHDIFF as default")

            # æ„å»ºæ¯”å¯¹é€‰é¡¹
            diff_options = {
                "algorithm": algorithm,
                "key_columns": key_columns,
                "where": config.get("source_filter"),
                "update_column": config.get("update_column"),
                "threaded": True,
                "max_threadpool_size": config.get("threads", 1)
            }
            
            # æ·»åŠ åˆ†æ®µæ¯”å¯¹å‚æ•°ï¼ˆä»…å¯¹ HASHDIFF ç®—æ³•æœ‰æ•ˆï¼‰
            if algorithm == Algorithm.HASHDIFF:
                # bisection_factor: æ¯æ¬¡è¿­ä»£çš„æ®µæ•°ï¼Œé»˜è®¤ 32
                if config.get("bisection_factor"):
                    diff_options["bisection_factor"] = config.get("bisection_factor")
                    self.logger.info(f"ğŸ“Š Using bisection_factor: {diff_options['bisection_factor']}")
                
                # bisection_threshold: æœ€å°åˆ†æ®µé˜ˆå€¼ï¼Œé»˜è®¤ 16384
                if config.get("bisection_threshold"):
                    diff_options["bisection_threshold"] = config.get("bisection_threshold")
                    self.logger.info(f"ğŸ“Š Using bisection_threshold: {diff_options['bisection_threshold']}")

            # æ·»åŠ  extra_columnsï¼ˆæ¯”è¾ƒçš„åˆ—ï¼‰
            compare_columns = config.get("compare_columns") or config.get("columns_to_compare")
            if compare_columns:
                if isinstance(compare_columns, str):
                    compare_columns = tuple(compare_columns.split(','))
                elif isinstance(compare_columns, list):
                    compare_columns = tuple(compare_columns)
                diff_options["extra_columns"] = compare_columns
            
            # æ·»åŠ æ•°æ®ç±»å‹å¤„ç†å‚æ•°
            if config.get("float_tolerance") is not None:
                diff_options["float_tolerance"] = config.get("float_tolerance")
                self.logger.info(f"ğŸ”¢ Float tolerance enabled: {diff_options['float_tolerance']}")
            
            if config.get("timestamp_precision"):
                diff_options["timestamp_precision"] = config.get("timestamp_precision")
                self.logger.info(f"â° Timestamp precision enabled: {diff_options['timestamp_precision']}")
            
            if config.get("json_comparison_mode"):
                diff_options["json_comparison_mode"] = config.get("json_comparison_mode")
                self.logger.info(f"ğŸ“„ JSON comparison mode enabled: {diff_options['json_comparison_mode']}")
            
            if config.get("column_remapping"):
                diff_options["column_remapping"] = config.get("column_remapping")
                diff_options["case_sensitive_remapping"] = config.get("case_sensitive_remapping", True)
                self.logger.info(f"ğŸ”„ Column remapping enabled: {diff_options['column_remapping']}")

            # ç§»é™¤å€¼ä¸º None çš„é€‰é¡¹
            diff_options = {k: v for k, v in diff_options.items() if v is not None}

            # åº”ç”¨é‡‡æ ·é€»è¾‘
            sampling_applied = False
            sample_size = None
            
            # ç°åœ¨ data-diff æ”¯æŒé‡‡æ ·äº†ï¼
            if config.get("enable_sampling", False):
                # æ„å»ºé‡‡æ ·é…ç½®
                sampling_config = SamplingConfig(
                    enabled=True,
                    confidence_level=config.get("sampling_confidence", 0.95),
                    margin_of_error=config.get("sampling_margin_of_error", 0.01),
                    min_sample_size=config.get("min_sample_size", 1000),
                    max_sample_size=config.get("max_sample_size", 1000000),
                    auto_sample_threshold=config.get("auto_sample_threshold", 100000)
                )
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨é‡‡æ ·ï¼ˆåŸºäºè¡Œæ•°ï¼‰
                source_count = config.get("_source_count", 0)
                target_count = config.get("_target_count", 0)
                max_row_count = max(source_count, target_count)
                
                # å¤„ç†ç™¾åˆ†æ¯”é‡‡æ ·
                if config.get("sampling_percent"):
                    sampling_applied = True
                    sampling_percent = config["sampling_percent"]
                    config["_sampling_applied"] = True
                    config["_sampling_percent"] = sampling_percent
                    
                    # æ·»åŠ é‡‡æ ·å‚æ•°åˆ° diff_options
                    diff_options["sampling_percent"] = sampling_percent
                    diff_options["sampling_method"] = config.get("sampling_method", "DETERMINISTIC")
                    
                    self.logger.info(f"ğŸ“Š Percentage sampling enabled: {sampling_percent}% (method: {config.get('sampling_method', 'DETERMINISTIC')})")
                    
                # å¤„ç†å›ºå®šå¤§å°é‡‡æ ·
                elif config.get("sample_size") and config["sample_size"] > 0:
                    # ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®šäº†é‡‡æ ·å¤§å°
                    sampling_applied = True
                    sample_size = config["sample_size"]
                    config["_sampling_applied"] = True
                    config["_actual_sample_size"] = sample_size
                    
                    # æ·»åŠ é‡‡æ ·å‚æ•°åˆ° diff_options
                    diff_options["sample_size"] = sample_size
                    diff_options["sampling_method"] = config.get("sampling_method", "DETERMINISTIC")
                    
                    self.logger.info(f"ğŸ“Š Fixed size sampling enabled: {sample_size} rows (method: {config.get('sampling_method', 'DETERMINISTIC')})")
                    
                # è‡ªåŠ¨é‡‡æ ·é€»è¾‘
                else:
                    should_sample, calculated_sample_size = self.sampling_engine.should_use_sampling(
                        max_row_count,
                        sampling_config
                    )
                    
                    if should_sample and calculated_sample_size:
                        # è‡ªåŠ¨è®¡ç®—çš„é‡‡æ ·å¤§å°
                        sample_size = calculated_sample_size
                        
                        # æ›´æ–°æ¯”å¯¹é€‰é¡¹ - ç°åœ¨ data-diff æ”¯æŒé‡‡æ ·äº†ï¼
                        sampling_applied = True
                        config["_sampling_applied"] = True
                        config["_actual_sample_size"] = sample_size
                        config["_sampling_config"] = {
                            "confidence_level": sampling_config.confidence_level,
                            "margin_of_error": sampling_config.margin_of_error
                        }
                        
                        # æ·»åŠ é‡‡æ ·å‚æ•°åˆ° diff_options
                        diff_options["sample_size"] = sample_size
                        diff_options["sampling_method"] = config.get("sampling_method", "DETERMINISTIC")
                        
                        self.logger.info(f"ğŸ“Š Auto sampling enabled: {sample_size} rows (confidence: {sampling_config.confidence_level*100}%, margin: {sampling_config.margin_of_error*100}%)")

            self.logger.info(f"ğŸ“‹ Executing diff_tables with options: {diff_options}")
            self.logger.info(f"ğŸš€ Starting actual table comparison (this will execute SQL queries)...")

            # æ‰§è¡Œæ¯”å¯¹
            diff_result = diff_tables(
                source_table_segment,
                target_table_segment,
                **diff_options
            )

            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            self.logger.info(f"âœ… Data-diff job {job_id} completed in {execution_time}s")
            self.logger.info(f"ğŸ“Š SQL execution finished, processing results...")

            # æ”¶é›†åˆ—çº§ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            column_statistics = None
            if config.get("enable_column_statistics", False) and ColumnStatisticsCollector:
                self.logger.info(f"ğŸ“Š Collecting column-level statistics...")
                try:
                    column_statistics = await self._collect_column_statistics(
                        source_table_segment,
                        target_table_segment,
                        config
                    )
                except Exception as stats_error:
                    self.logger.warning(f"Failed to collect column statistics: {stats_error}")
                    # ç»§ç»­å¤„ç†ï¼Œç»Ÿè®¡å¤±è´¥ä¸åº”å½±å“ä¸»è¦æ¯”å¯¹ç»“æœ

            # æ‰§è¡Œæ—¶é—´çº¿åˆ†æï¼ˆå¦‚æœé…ç½®äº†æ—¶é—´åˆ—ï¼‰
            timeline_analysis = None
            if config.get("timeline_column") and TimelineAnalyzer:
                self.logger.info(f"ğŸ“ˆ Performing timeline analysis on column: {config['timeline_column']}")
                self.logger.info(f"ğŸ” TimelineAnalyzer available: {TimelineAnalyzer is not None}")
                try:
                    timeline_analysis = await self._analyze_timeline(
                        source_table_segment,
                        target_table_segment,
                        diff_result,
                        config
                    )
                    self.logger.info(f"ğŸ¯ Timeline analysis result: {timeline_analysis is not None}")
                    if timeline_analysis:
                        self.logger.info(f"ğŸ“Š Timeline analysis keys: {list(timeline_analysis.keys()) if isinstance(timeline_analysis, dict) else 'not a dict'}")
                except Exception as timeline_error:
                    self.logger.warning(f"âš ï¸ Failed to perform timeline analysis: {timeline_error}")
                    import traceback
                    self.logger.warning(f"Timeline traceback: {traceback.format_exc()}")
                    # ç»§ç»­å¤„ç†ï¼Œæ—¶é—´çº¿åˆ†æå¤±è´¥ä¸åº”å½±å“ä¸»è¦æ¯”å¯¹ç»“æœ
            else:
                self.logger.info(f"ğŸš« Timeline analysis skipped. Column: {config.get('timeline_column')}, Analyzer: {TimelineAnalyzer is not None}")

            # å¤„ç†ç»“æœ
            return self._process_datadiff_result(
                diff_result, config, job_id, start_time, end_time, 
                column_statistics=column_statistics,
                timeline_analysis=timeline_analysis
            )

        except Exception as e:
            self.logger.error(f"Data-diff execution for job {job_id} failed: {e}", exc_info=True)
            raise Exception(f"Data-diff execution failed: {str(e)}")

    def _process_datadiff_result(
        self,
        diff_result: Any,
        config: Dict[str, Any],
        job_id: str,
        start_time: datetime,
        end_time: datetime,
        column_statistics: Optional[Dict[str, Any]] = None,
        timeline_analysis: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç† data-diff ç»“æœ
        """
        try:
            execution_time = (end_time - start_time).total_seconds()

            # è·å–æºè¡¨å’Œç›®æ ‡è¡¨çš„è¡Œæ•°ä¿¡æ¯
            total_rows_source = 0
            total_rows_target = 0

            try:
                # æ­£ç¡®çš„æ–¹æ³•ï¼šä½¿ç”¨ DiffResultWrapper çš„ _get_stats æ–¹æ³•
                if hasattr(diff_result, '_get_stats'):
                    stats = diff_result._get_stats()
                    total_rows_source = stats.table1_count
                    total_rows_target = stats.table2_count
                    self.logger.info(f"ğŸ“Š ä» DiffResultWrapper è·å–è¡Œæ•°: æºè¡¨={total_rows_source}, ç›®æ ‡è¡¨={total_rows_target}")
                elif hasattr(diff_result, 'info_tree') and hasattr(diff_result.info_tree, 'info'):
                    # å¤‡é€‰æ–¹æ³•ï¼šç›´æ¥ä» info_tree è·å–
                    rowcounts = diff_result.info_tree.info.rowcounts
                    if rowcounts:
                        total_rows_source = rowcounts.get(1, 0)
                        total_rows_target = rowcounts.get(2, 0)
                        self.logger.info(f"ğŸ“Š ä» info_tree è·å–è¡Œæ•°: æºè¡¨={total_rows_source}, ç›®æ ‡è¡¨={total_rows_target}")
                else:
                    self.logger.warning("Unable to find row count information in diff_result")
            except Exception as e:
                self.logger.warning(f"Unable to get row counts from diff_result: {e}")

            # è·å–å·®å¼‚æ•°é‡
            total_differences = 0
            try:
                if hasattr(diff_result, '__len__'):
                    total_differences = len(diff_result)
                elif hasattr(diff_result, 'summary') and 'total_differences' in diff_result.summary:
                    total_differences = diff_result.summary['total_differences']
                else:
                    # å°è¯•å°†diff_resultè½¬æ¢ä¸ºåˆ—è¡¨å¹¶è®¡ç®—é•¿åº¦
                    differences_list = list(diff_result)
                    total_differences = len(differences_list)
            except Exception as e:
                self.logger.warning(f"Unable to get total differences from diff_result: {e}")

            # æ„å»ºå·®å¼‚ç»Ÿè®¡ä¿¡æ¯
            differences = {
                "total_differences": total_differences,
                "missing_in_target": 0,
                "missing_in_source": 0,
                "value_differences": 0
            }

            # å°è¯•è·å–æ›´è¯¦ç»†çš„å·®å¼‚ä¿¡æ¯
            try:
                if hasattr(diff_result, 'diff_count_by_type'):
                    diff_counts = diff_result.diff_count_by_type
                    differences["missing_in_target"] = diff_counts.get('missing_in_b', 0)
                    differences["missing_in_source"] = diff_counts.get('missing_in_a', 0)
                    differences["value_differences"] = diff_counts.get('value_different', 0)
            except Exception as e:
                self.logger.warning(f"Unable to get detailed difference counts: {e}")

            # å¦‚æœä» diff_result è·å–çš„è¡Œæ•°ä¸æ­£ç¡®ï¼Œä½¿ç”¨é…ç½®ä¸­ä¿å­˜çš„è¡Œæ•°
            if (total_rows_source == 0 or total_rows_target == 0) and config.get('_source_count'):
                total_rows_source = config.get('_source_count', 0)
                total_rows_target = config.get('_target_count', 0)
                self.logger.info(f"ğŸ“Š ä½¿ç”¨é…ç½®ä¸­çš„è¡Œæ•°: æºè¡¨={total_rows_source}, ç›®æ ‡è¡¨={total_rows_target}")
            
            # è®¡ç®—æ¯”å¯¹çš„è¡Œæ•°
            if total_rows_source == 0 and total_rows_target == 0:
                rows_compared = max(total_differences, 1) # ç¡®ä¿ä¸ä¼šé™¤ä»¥é›¶
            else:
                rows_compared = max(total_rows_source, total_rows_target)  # ä½¿ç”¨ max è€Œä¸æ˜¯ min
                if rows_compared == 0:
                    rows_compared = 1  # ç¡®ä¿ä¸ä¼šé™¤ä»¥é›¶

            # è®¡ç®—åŒ¹é…ç‡
            match_rate = 1.0
            if rows_compared > 0:
                match_rate = 1 - (total_differences / rows_compared)

            # è·å–æ ·æœ¬å·®å¼‚å¹¶è¿›è¡Œåˆ†ç±»
            sample_differences = []
            classified_differences = []
            classification_summary = {}
            
            try:
                # å°è¯•è·å–ä¸€äº›æ ·æœ¬å·®å¼‚
                self.logger.info(f"ğŸ” Attempting to extract sample differences from diff_result")
                self.logger.info(f"diff_result type: {type(diff_result)}")
                self.logger.info(f"diff_result attributes: {dir(diff_result)}")
                
                if hasattr(diff_result, 'diffs'):
                    self.logger.info(f"âœ… diff_result has 'diffs' attribute")
                    diffs_sample = list(diff_result.diffs)[:100]  # è·å–æ›´å¤šæ ·æœ¬ç”¨äºåˆ†ç±»
                    self.logger.info(f"ğŸ“Š Got {len(diffs_sample)} sample differences")
                else:
                    # å¦‚æœdiff_resultæ˜¯ä¸€ä¸ªç”Ÿæˆå™¨æˆ–è¿­ä»£å™¨
                    self.logger.info(f"ğŸ”„ diff_result doesn't have 'diffs' attribute, trying to iterate")
                    diffs_sample = []
                    for i, diff in enumerate(diff_result):
                        if i >= 100:
                            break
                        diffs_sample.append(diff)
                    self.logger.info(f"ğŸ“Š Got {len(diffs_sample)} differences by iterating")
                    
                    # åˆå§‹åŒ–å·®å¼‚åˆ†ç±»å™¨
                    if DifferenceClassifier:
                        classifier_config = {
                            'case_sensitive': config.get('case_sensitive', True),
                            'numeric_tolerance': config.get('tolerance', 0.0),
                            'treat_null_as_critical': config.get('treat_null_as_critical', False)
                        }
                        classifier = DifferenceClassifier(classifier_config)
                        
                        # å‡†å¤‡å·®å¼‚æ•°æ®ç”¨äºåˆ†ç±»
                        diffs_for_classification = []
                    
                    for i, diff in enumerate(diffs_sample):
                        # è®°å½•å·®å¼‚çš„åŸå§‹æ ¼å¼
                        if i == 0:
                            self.logger.info(f"ğŸ” First diff sample type: {type(diff)}")
                            self.logger.info(f"ğŸ” First diff sample: {diff}")
                        
                        # å¤„ç†ä¸åŒçš„å·®å¼‚æ ¼å¼
                        if isinstance(diff, dict):
                            diff_type = diff.get('diff_type', 'unknown')
                            key = diff.get('key', {})
                            source_row = diff.get('a_values', None)
                            target_row = diff.get('b_values', None)
                        elif isinstance(diff, tuple) and len(diff) >= 2:
                            # data-diff è¿”å›çš„æ ¼å¼: ('+'/'-'/('!', ...)), (row_data))
                            diff_sign = diff[0]
                            row_data = diff[1]
                            
                            # å°†å…ƒç»„è½¬æ¢ä¸ºå­—å…¸
                            column_names = config.get('_column_names', [])
                            if not column_names:
                                # å¦‚æœæ²¡æœ‰åˆ—åä¿¡æ¯ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤å€¼
                                self.logger.warning("âš ï¸ No column names in config, using default")
                                column_names = ['product_id', 'product_name', 'price', 'quantity', 'created_at']
                            if config.get('key_columns'):
                                key_col = config['key_columns'][0]
                                key_index = column_names.index(key_col) if key_col in column_names else 0
                                key = {key_col: row_data[key_index]}
                            else:
                                key = {'product_id': row_data[0]}
                            
                            if diff_sign == '-':
                                diff_type = 'missing_in_target'
                                source_row = dict(zip(column_names, row_data))
                                target_row = None
                            elif diff_sign == '+':
                                diff_type = 'missing_in_source'
                                source_row = None
                                target_row = dict(zip(column_names, row_data))
                            elif isinstance(diff_sign, tuple) and diff_sign[0] == '!':
                                # å€¼å·®å¼‚: ('!', column_index)
                                diff_type = 'value_different'
                                # TODO: å¤„ç†å€¼å·®å¼‚çš„æƒ…å†µ
                                continue
                            else:
                                self.logger.warning(f"âš ï¸ Unknown diff sign: {diff_sign}")
                                continue
                        else:
                            # å…¶ä»–æœªçŸ¥æ ¼å¼
                            self.logger.warning(f"âš ï¸ Unknown diff format: {type(diff)}, content: {diff}")
                            continue

                        sample_diff = {
                            "type": diff_type,
                            "key": key
                        }

                        if source_row:
                            sample_diff["source_row"] = source_row

                        if target_row:
                            sample_diff["target_row"] = target_row

                        if diff_type == 'value_different' and source_row and target_row:
                            # æ‰¾å‡ºä¸åŒçš„åˆ—
                            differing_columns = []
                            columns_diff = {}
                            for col in source_row:
                                if col in target_row and source_row[col] != target_row[col]:
                                    differing_columns.append(col)
                                    columns_diff[col] = {
                                        'source': source_row[col],
                                        'target': target_row[col]
                                    }
                            sample_diff["differing_columns"] = differing_columns
                            
                            # å‡†å¤‡åˆ†ç±»æ•°æ®
                            if DifferenceClassifier and columns_diff:
                                diff_for_classification = key.copy()
                                diff_for_classification['columns'] = columns_diff
                                diffs_for_classification.append(diff_for_classification)

                        # åªä¿ç•™å‰10ä¸ªä½œä¸ºæ ·æœ¬
                        if i < 10:
                            sample_differences.append(sample_diff)
                    
                    # æ‰§è¡Œå·®å¼‚åˆ†ç±»
                    if DifferenceClassifier and diffs_for_classification:
                        # è·å–åˆ—ç±»å‹ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        column_types = {}
                        try:
                            if hasattr(source_table_segment, 'get_schema'):
                                schema = source_table_segment.get_schema()
                                for col_name, col_info in schema.items():
                                    column_types[col_name] = str(col_info.data_type) if hasattr(col_info, 'data_type') else str(col_info)
                        except:
                            pass
                        
                        # åˆ†ç±»å·®å¼‚
                        classified = classifier.classify_differences(diffs_for_classification, column_types)
                        classified_differences = [c.to_dict() for c in classified[:20]]  # ä¿ç•™å‰20ä¸ªåˆ†ç±»ç»“æœ
                        
                        # ç”Ÿæˆåˆ†ç±»æ‘˜è¦
                        classification_summary = classifier.generate_summary(classified)
                        
            except Exception as e:
                self.logger.warning(f"Unable to get or classify sample differences: {e}")

            # æ„å»ºå®Œæ•´çš„ç»“æœ
            result = {
                "status": "completed",
                "job_id": job_id,
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "execution_time_seconds": execution_time,
                "config": config,
                "statistics": {
                    "source_table": config["source_table"],
                    "target_table": config["target_table"],
                    "total_rows_source": total_rows_source,
                    "total_rows_target": total_rows_target,
                    "rows_compared": rows_compared,
                    "differences": differences,
                    "match_rate": match_rate
                },
                "sample_differences": sample_differences,
                "summary": {
                    "has_differences": total_differences > 0,
                    "match_percentage": round(match_rate * 100, 2),
                    "data_quality_score": "Good" if match_rate > 0.95 else "Fair" if match_rate > 0.8 else "Poor",
                    "total_rows": rows_compared,
                    "rows_matched": rows_compared - total_differences if rows_compared >= total_differences else 0,
                    "rows_different": total_differences,
                    "match_rate": round(match_rate * 100, 2),
                    "execution_time": execution_time
                }
            }
            
            # æ·»åŠ å·®å¼‚åˆ†ç±»ç»“æœ
            if classified_differences or classification_summary:
                result["difference_classification"] = {
                    "classified_samples": classified_differences,
                    "summary": classification_summary
                }            # æ·»åŠ ç±»å‹è­¦å‘Šä¿¡æ¯
            type_warnings = config.get("_type_warnings", [])
            ignored_columns_details = config.get("_ignored_columns_details", [])

            if type_warnings:
                # æå–è¢«å¿½ç•¥çš„åˆ—å
                ignored_columns = [col.get('column_name', '') for col in ignored_columns_details]

                # æ£€æŸ¥æ¯”å¯¹é…ç½®ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰å‚ä¸æ¯”å¯¹çš„åˆ—è¢«å¿½ç•¥
                compare_columns = config.get("compare_columns") or config.get("columns_to_compare")
                compare_columns_set = set()
                if compare_columns:
                    if isinstance(compare_columns, str):
                        compare_columns_set = set(col.strip() for col in compare_columns.split(','))
                    elif isinstance(compare_columns, list):
                        compare_columns_set = set(compare_columns)

                # åˆ¤æ–­æ˜¯å¦æœ‰å‚ä¸æ¯”å¯¹çš„åˆ—è¢«å¿½ç•¥
                has_ignored_comparison_columns = False
                if compare_columns_set:
                    # æŒ‡å®šäº†æ¯”å¯¹åˆ—ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰è¢«å¿½ç•¥çš„
                    has_ignored_comparison_columns = any(col.get('column_name') in compare_columns_set for col in ignored_columns_details)
                elif compare_columns is None:
                    # æ²¡æœ‰æŒ‡å®šæ¯”å¯¹åˆ—ï¼ˆæ¯”å¯¹æ‰€æœ‰åˆ—ï¼‰ï¼Œæ‰€ä»¥ä»»ä½•è¢«å¿½ç•¥çš„åˆ—éƒ½å½±å“æ¯”å¯¹
                    has_ignored_comparison_columns = bool(ignored_columns_details)
                else:
                    # compare_columns æ˜¯ç©ºæ•°ç»„ï¼Œè¡¨ç¤ºä¸æ¯”å¯¹ä»»ä½•åˆ—ï¼Œä¸æ”¯æŒçš„åˆ—ä¸å½±å“æ¯”å¯¹
                    has_ignored_comparison_columns = False

                # ç”Ÿæˆç”¨æˆ·å»ºè®®ï¼ˆéœ€è¦ä¼ å…¥source_configå’Œtarget_configï¼Œæš‚æ—¶ä»configè·å–ï¼‰
                user_suggestions = self._generate_user_suggestions(
                    type_warnings,
                    ignored_columns,
                    config.get('_source_config', {}),
                    config.get('_target_config', {})
                )

                result["warnings"] = {
                    "unsupported_types": type_warnings,
                    "message": "ğŸš¨ ä¸¥é‡é”™è¯¯ï¼šæ£€æµ‹åˆ°ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ï¼Œè¿™äº›å­—æ®µè¢«å®Œå…¨å¿½ç•¥ï¼æ¯”å¯¹ç»“æœä¸å¯é ï¼Œå¯èƒ½æ˜¾ç¤º100%åŒ¹é…ä½†å®é™…æœ‰å·®å¼‚ï¼" if has_ignored_comparison_columns else "âš ï¸ æ£€æµ‹åˆ°è¡¨ä¸­åŒ…å«ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ï¼Œä½†è¿™äº›å­—æ®µæœªå‚ä¸æ¯”å¯¹ï¼Œä¸å½±å“æ¯”å¯¹ç»“æœã€‚",
                    "severity": "critical" if has_ignored_comparison_columns else "warning",
                    "impact": "æ¯”å¯¹ç»“æœä¸å¯ä¿¡ï¼Œä¸åº”åŸºäºæ­¤ç»“æœåšå†³ç­–" if has_ignored_comparison_columns else "ä¸å½±å“æ¯”å¯¹ç»“æœï¼Œä»…ä¾›å‚è€ƒ",
                    "recommendation": "1) å¯ç”¨ä¸¥æ ¼ç±»å‹æ£€æŸ¥æ¨¡å¼ç«‹å³å¤±è´¥ 2) é¢„å¤„ç†æ•°æ®è½¬æ¢ç±»å‹ 3) ä»æ¯”å¯¹ä¸­æ’é™¤è¿™äº›å­—æ®µ" if has_ignored_comparison_columns else "å¦‚éœ€æ¯”å¯¹è¿™äº›å­—æ®µï¼Œè¯·å…ˆè¿›è¡Œæ•°æ®ç±»å‹è½¬æ¢",
                    "ignored_columns": ignored_columns_details,  # æ–°å¢ï¼šè¯¦ç»†çš„è¢«å¿½ç•¥å­—æ®µåˆ—è¡¨
                    "user_suggestions": user_suggestions  # æ–°å¢ï¼šç”¨æˆ·å»ºè®®
                }
                # åªæœ‰å½“å®é™…å‚ä¸æ¯”å¯¹çš„å­—æ®µæœ‰é—®é¢˜æ—¶ï¼Œæ‰è®¾ç½®ä¸ºå¤±è´¥
                if has_ignored_comparison_columns:
                    result["summary"]["data_quality_score"] = "Failed"  # æ”¹ä¸º Failed è€Œä¸æ˜¯ Poor
                    result["summary"]["incomplete_comparison"] = True
                    result["summary"]["comparison_invalid"] = True  # æ–°å¢ï¼šæ ‡è®°æ¯”å¯¹æ— æ•ˆ
                    result["statistics"]["warning"] = "âš ï¸ æ¯”å¯¹å¤±è´¥ - å…³é”®å­—æ®µè¢«å¿½ç•¥ï¼Œç»“æœä¸å¯ä¿¡"
                    result["statistics"]["reliability"] = "unreliable"  # æ–°å¢ï¼šå¯é æ€§æ ‡è®°
                else:
                    # è¡¨ä¸­æœ‰ä¸æ”¯æŒçš„å­—æ®µï¼Œä½†æ²¡æœ‰å‚ä¸æ¯”å¯¹ï¼Œæ¯”å¯¹ç»“æœä»ç„¶å¯é 
                    result["summary"]["data_quality_score"] = result["summary"].get("data_quality_score", "Good")
                    result["statistics"]["reliability"] = "reliable"

                result["summary"]["ignored_columns_count"] = len(ignored_columns_details)
                result["summary"]["ignored_columns_list"] = [f"{col['table_name']}.{col['column_name']} ({col['data_type']})" for col in ignored_columns_details]
                result["statistics"]["ignored_columns_details"] = ignored_columns_details  # åœ¨ç»Ÿè®¡ä¸­ä¹ŸåŒ…å«è¯¦ç»†ä¿¡æ¯

            # æ·»åŠ åˆ—çº§ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if column_statistics:
                result["column_statistics"] = column_statistics
                # åœ¨æ‘˜è¦ä¸­æ·»åŠ ç»Ÿè®¡æ¦‚è§ˆ
                if "comparison" in column_statistics and "summary" in column_statistics["comparison"]:
                    stats_summary = column_statistics["comparison"]["summary"]
                    result["summary"]["column_statistics_summary"] = {
                        "total_columns": stats_summary.get("total_columns", 0),
                        "columns_with_differences": stats_summary.get("columns_with_differences", 0),
                        "has_warnings": len(column_statistics["comparison"].get("warnings", [])) > 0
                    }

            # æ·»åŠ æ—¶é—´çº¿åˆ†æä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if timeline_analysis:
                result["timeline_analysis"] = timeline_analysis
                # åœ¨æ‘˜è¦ä¸­æ·»åŠ æ—¶é—´çº¿æ¦‚è§ˆ
                if "summary" in timeline_analysis:
                    timeline_summary = timeline_analysis["summary"]
                    result["summary"]["timeline_summary"] = {
                        "time_column": timeline_summary.get("time_column"),
                        "total_time_periods": timeline_summary.get("total_time_periods", 0),
                        "average_match_rate": timeline_summary.get("average_match_rate", 100),
                        "has_patterns": len(timeline_analysis.get("patterns", [])) > 0
                    }
            
            # å°è¯•ç‰©åŒ–ç»“æœåˆ°æ•°æ®åº“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.result_materializer and config.get('materialize_results', True):  # é»˜è®¤å¯ç”¨
                try:
                    # å‡†å¤‡ç‰©åŒ–æ•°æ®
                    materialization_data = {
                        'config': config,
                        'summary': result.get('summary', {}),
                        'differences': result.get('sample_differences', []),
                        'column_stats': column_statistics,
                        'timeline_analysis': timeline_analysis,
                        'performance_metrics': {
                            'execution_time': execution_time,
                            'rows_per_second': total_rows_source / execution_time if execution_time > 0 else 0,
                            'memory_usage_mb': 0  # TODO: å®ç°å†…å­˜ä½¿ç”¨ç›‘æ§
                        },
                        'start_time': start_time,
                        'end_time': end_time
                    }
                    
                    # æ‰§è¡Œç‰©åŒ–
                    success = self.result_materializer.materialize_results(job_id, materialization_data)
                    if success:
                        result['summary']['result_materialized'] = True
                        self.logger.info(f"Results successfully materialized for comparison {job_id}")
                    else:
                        self.logger.warning(f"Failed to materialize results for comparison {job_id}")
                except Exception as e:
                    self.logger.error(f"Error materializing results: {e}")

            return result

        except Exception as e:
            self.logger.error(f"Failed to process data-diff result: {e}", exc_info=True)
            return {
                "status": "error",
                "job_id": job_id,
                "message": f"Failed to process result: {str(e)}",
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "raw_result": str(diff_result)[:1000] if diff_result else "None"  # æ·»åŠ åŸå§‹ç»“æœçš„æˆªæ–­ç‰ˆæœ¬ä»¥ä¾¿è°ƒè¯•
            }

    async def get_comparison_status(self, job_id: str) -> Dict[str, Any]:
        """
        è·å–æ¯”å¯¹ä»»åŠ¡çŠ¶æ€

        Args:
            job_id: ä»»åŠ¡ID

        Returns:
            ä»»åŠ¡çŠ¶æ€
        """
        if job_id not in self.active_comparisons:
            return {
                "status": "not_found",
                "message": f"Comparison job {job_id} not found"
            }

        job_info = self.active_comparisons[job_id]

        return {
            "job_id": job_id,
            "status": job_info["status"],
            "start_time": job_info["start_time"].isoformat(),
            "end_time": job_info.get("end_time", {}).isoformat() if job_info.get("end_time") else None,
            "config": job_info["config"],
            "error": job_info.get("error")
        }

    def list_active_comparisons(self) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºæ´»åŠ¨çš„æ¯”å¯¹ä»»åŠ¡

        Returns:
            ä»»åŠ¡åˆ—è¡¨
        """
        comparisons = []
        for job_id, job_info in self.active_comparisons.items():
            comparisons.append({
                "job_id": job_id,
                "status": job_info["status"],
                "start_time": job_info["start_time"].isoformat(),
                "source_table": job_info["config"].get("source_table"),
                "target_table": job_info["config"].get("target_table")
            })
        return comparisons

    async def cancel_comparison(self, job_id: str) -> bool:
        """
        å–æ¶ˆæ¯”å¯¹ä»»åŠ¡

        Args:
            job_id: ä»»åŠ¡ID

        Returns:
            æ˜¯å¦æˆåŠŸå–æ¶ˆ
        """
        if job_id not in self.active_comparisons:
            return False

        job_info = self.active_comparisons[job_id]

        if job_info["status"] == "running":
            # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„ä»»åŠ¡å–æ¶ˆé€»è¾‘
            job_info["status"] = "cancelled"
            job_info["end_time"] = datetime.now()
            return True

        return False

    def cleanup_completed_comparisons(self, max_age_hours: int = 24) -> int:
        """
        æ¸…ç†å·²å®Œæˆçš„æ¯”å¯¹ä»»åŠ¡

        Args:
            max_age_hours: æœ€å¤§ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰

        Returns:
            æ¸…ç†çš„ä»»åŠ¡æ•°é‡
        """
        now = datetime.now()
        cleaned_count = 0

        jobs_to_remove = []
        for job_id, job_info in self.active_comparisons.items():
            if job_info["status"] in ["completed", "error", "cancelled"]:
                end_time = job_info.get("end_time", job_info["start_time"])
                age_hours = (now - end_time).total_seconds() / 3600

                if age_hours > max_age_hours:
                    jobs_to_remove.append(job_id)

        for job_id in jobs_to_remove:
            del self.active_comparisons[job_id]
            cleaned_count += 1

        self.logger.info(f"Cleaned up {cleaned_count} old comparison jobs")
        return cleaned_count

    async def _collect_column_statistics(
        self,
        source_table_segment: Any,
        target_table_segment: Any,
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ”¶é›†åˆ—çº§ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            source_table_segment: æºè¡¨æ®µ
            target_table_segment: ç›®æ ‡è¡¨æ®µ
            config: æ¯”å¯¹é…ç½®
            
        Returns:
            åŒ…å«åˆ—ç»Ÿè®¡çš„å­—å…¸
        """
        try:
            # åˆ›å»ºç»Ÿè®¡æ”¶é›†å™¨
            source_collector = ColumnStatisticsCollector(source_table_segment.database.dialect)
            target_collector = ColumnStatisticsCollector(target_table_segment.database.dialect)
            
            # è·å–éœ€è¦ç»Ÿè®¡çš„åˆ—
            source_schema = source_table_segment.get_schema()
            target_schema = target_table_segment.get_schema()
            
            # è½¬æ¢ä¸ºåˆ—ç±»å‹å…ƒç»„åˆ—è¡¨
            source_columns = []
            target_columns = []
            
            # è·å–å¤„ç†åçš„schema
            try:
                # è·å–è¡¨è·¯å¾„
                source_path = source_table_segment.table_path
                target_path = target_table_segment.table_path
                
                # è·å–æ¯”å¯¹åˆ—
                compare_columns = config.get('columns_to_compare') or config.get('compare_columns')
                if compare_columns and isinstance(compare_columns, list):
                    filter_columns = compare_columns
                else:
                    filter_columns = list(source_schema.keys())
                
                processed_source_schema = source_table_segment.database._process_table_schema(
                    source_path, source_schema, filter_columns
                )
                for col_name, col_type in processed_source_schema.items():
                    source_columns.append((col_name, col_type))
                    
                processed_target_schema = target_table_segment.database._process_table_schema(
                    target_path, target_schema, filter_columns
                )
                for col_name, col_type in processed_target_schema.items():
                    target_columns.append((col_name, col_type))
            except Exception as e:
                self.logger.warning(f"Failed to process schemas for statistics: {e}")
                return {"error": str(e)}
            
            # ç”Ÿæˆç»Ÿè®¡æŸ¥è¯¢SQL
            source_stats_sql = source_collector.generate_statistics_sql(
                config["source_table"],
                source_columns,
                sample_size=config.get("_actual_sample_size"),
                where_clause=config.get("source_filter")
            )
            
            target_stats_sql = target_collector.generate_statistics_sql(
                config["target_table"],
                target_columns,
                sample_size=config.get("_actual_sample_size"),
                where_clause=config.get("target_filter")
            )
            
            self.logger.debug(f"Source statistics SQL: {source_stats_sql}")
            self.logger.debug(f"Target statistics SQL: {target_stats_sql}")
            
            # æ‰§è¡Œç»Ÿè®¡æŸ¥è¯¢
            source_stats_result = None
            target_stats_result = None
            
            # ä½¿ç”¨æ•°æ®åº“è¿æ¥æ‰§è¡ŒæŸ¥è¯¢
            with source_table_segment.database.create_connection() as source_conn:
                with source_conn.cursor() as cursor:
                    cursor.execute(source_stats_sql)
                    source_stats_result = dict(zip(
                        [desc[0] for desc in cursor.description],
                        cursor.fetchone()
                    ))
                    
            with target_table_segment.database.create_connection() as target_conn:
                with target_conn.cursor() as cursor:
                    cursor.execute(target_stats_sql)
                    target_stats_result = dict(zip(
                        [desc[0] for desc in cursor.description],
                        cursor.fetchone()
                    ))
            
            # è§£æç»“æœ
            source_stats = source_collector.parse_statistics_result(
                source_stats_result, source_columns
            )
            target_stats = target_collector.parse_statistics_result(
                target_stats_result, target_columns
            )
            
            # æ¯”è¾ƒç»Ÿè®¡ä¿¡æ¯
            comparison = source_collector.compare_column_statistics(
                source_stats, target_stats
            )
            
            # ç”ŸæˆæŠ¥å‘Š
            report = source_collector.generate_statistics_report(
                source_stats, target_stats, comparison
            )
            
            return {
                "source_statistics": {col: stats.to_dict() for col, stats in source_stats.items()},
                "target_statistics": {col: stats.to_dict() for col, stats in target_stats.items()},
                "comparison": comparison,
                "report": report
            }
            
        except Exception as e:
            self.logger.error(f"Failed to collect column statistics: {e}", exc_info=True)
            return {"error": str(e)}

    async def _analyze_timeline(
        self,
        source_table_segment: Any,
        target_table_segment: Any,
        diff_result: Any,
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ—¶é—´çº¿åˆ†æ
        
        Args:
            source_table_segment: æºè¡¨æ®µ
            target_table_segment: ç›®æ ‡è¡¨æ®µ
            diff_result: æ¯”å¯¹ç»“æœ
            config: æ¯”å¯¹é…ç½®
            
        Returns:
            æ—¶é—´çº¿åˆ†æç»“æœ
        """
        try:
            time_column = config["timeline_column"]
            self.logger.info(f"ğŸ“ˆ Starting timeline analysis for column: {time_column}")
            
            # åˆ›å»ºæ—¶é—´çº¿åˆ†æå™¨
            analyzer = TimelineAnalyzer(time_column, source_table_segment.database.dialect)
            self.logger.info(f"âœ… TimelineAnalyzer created")
            
            # è·å–æ—¶é—´èŒƒå›´
            start_time = None
            end_time = None
            
            if config.get("timeline_start_time"):
                start_time = datetime.fromisoformat(config["timeline_start_time"])
            if config.get("timeline_end_time"):
                end_time = datetime.fromisoformat(config["timeline_end_time"])
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œä»æ•°æ®ä¸­è‡ªåŠ¨æ£€æµ‹
            if not start_time or not end_time:
                # è·å–æºè¡¨æ—¶é—´èŒƒå›´
                time_range_sql = f"""
                SELECT 
                    MIN({analyzer.dialect.quote(time_column)}) as min_time,
                    MAX({analyzer.dialect.quote(time_column)}) as max_time
                FROM {config["source_table"]}
                """
                
                if config.get("source_filter"):
                    time_range_sql += f" WHERE {config['source_filter']}"
                
                with source_table_segment.database.create_connection() as conn:
                    with conn.cursor() as cursor:
                        cursor.execute(time_range_sql)
                        result = cursor.fetchone()
                        if result:
                            if not start_time and result[0]:
                                start_time = result[0]
                            if not end_time and result[1]:
                                end_time = result[1]
            
            if not start_time or not end_time:
                return {"error": "Could not determine time range for analysis"}
            
            # ç¡®å®šæ—¶é—´çª—å£å¤§å°
            window_size = analyzer.determine_window_size(
                start_time, end_time,
                target_buckets=config.get("timeline_buckets", 20)
            )
            
            # åˆ›å»ºæ—¶é—´æ¡¶
            time_buckets = analyzer.create_time_buckets(start_time, end_time, window_size)
            
            # æ”¶é›†å·®å¼‚æ ·æœ¬ï¼ˆé™åˆ¶æ•°é‡ä»¥æé«˜æ€§èƒ½ï¼‰
            differences = []
            try:
                diff_count = 0
                max_diffs = config.get("timeline_max_differences", 10000)
                
                for diff in diff_result:
                    if diff_count >= max_diffs:
                        break
                    
                    # è½¬æ¢å·®å¼‚æ ¼å¼ï¼ˆå¤„ç†å…ƒç»„æ ¼å¼ï¼‰
                    if isinstance(diff, tuple) and len(diff) >= 2:
                        diff_sign = diff[0]
                        row_data = diff[1]
                        
                        # è·å–åˆ—å
                        column_names = config.get('_column_names', [])
                        if not column_names:
                            self.logger.warning("âš ï¸ No column names for timeline analysis")
                            continue
                        
                        # å°†å…ƒç»„è½¬æ¢ä¸ºå­—å…¸
                        row_dict = dict(zip(column_names, row_data))
                        
                        if diff_sign == '-':
                            diff_dict = {
                                "type": "missing_in_target",
                                "key": {config['key_columns'][0]: row_data[0]},
                                "source_row": row_dict,
                                "target_row": None
                            }
                        elif diff_sign == '+':
                            diff_dict = {
                                "type": "missing_in_source",
                                "key": {config['key_columns'][0]: row_data[0]},
                                "source_row": None,
                                "target_row": row_dict
                            }
                        else:
                            continue
                    elif isinstance(diff, dict):
                        # å¦‚æœå·²ç»æ˜¯å­—å…¸æ ¼å¼
                        diff_dict = {
                            "type": diff.get("diff_type", "unknown"),
                            "key": diff.get("key", {}),
                            "source_row": diff.get("a_values"),
                            "target_row": diff.get("b_values")
                        }
                    else:
                        self.logger.warning(f"âš ï¸ Unknown diff format in timeline: {type(diff)}")
                        continue
                    
                    differences.append(diff_dict)
                    diff_count += 1
                    
            except Exception as e:
                self.logger.warning(f"Error collecting differences for timeline: {e}")
            
            # åˆ†æå·®å¼‚
            timeline_data = analyzer.analyze_differences(
                differences, time_buckets, time_column
            )
            
            # ç”Ÿæˆæ—¶é—´çº¿æŠ¥å‘Š
            timeline_report = analyzer.generate_timeline_report(
                timeline_data,
                config["source_table"],
                config["target_table"]
            )
            
            self.logger.info(f"âœ… Timeline analysis completed successfully")
            self.logger.info(f"ğŸ“ˆ Timeline report summary: {timeline_report.get('summary', {})}")
            
            return timeline_report
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to analyze timeline: {e}", exc_info=True)
            import traceback
            self.logger.error(f"Traceback: {traceback.format_exc()}")
            return {"error": str(e)}

    def _generate_user_suggestions(self,
                                  type_warnings: List[str],
                                  ignored_columns: List[str],
                                  source_config: Dict[str, Any],
                                  target_config: Dict[str, Any]) -> List[str]:
        """
        ä¸ºç”¨æˆ·ç”Ÿæˆæœ‰ç”¨çš„å»ºè®®å’Œè§£å†³æ–¹æ¡ˆ

        Args:
            type_warnings: ç±»å‹è­¦å‘Šåˆ—è¡¨
            ignored_columns: è¢«å¿½ç•¥çš„åˆ—
            source_config: æºæ•°æ®åº“é…ç½®
            target_config: ç›®æ ‡æ•°æ®åº“é…ç½®

        Returns:
            å»ºè®®åˆ—è¡¨
        """
        suggestions = []

        source_type = source_config.get('database_type', 'unknown')
        target_type = target_config.get('database_type', 'unknown')

        if type_warnings:
            suggestions.append(f"ğŸ” å‘ç° {len(type_warnings)} ä¸ªç±»å‹å…¼å®¹æ€§é—®é¢˜")

            if ignored_columns:
                suggestions.append(f"ğŸ“‹ ä»¥ä¸‹åˆ—è¢«å¿½ç•¥: {', '.join(ignored_columns)}")

            # é’ˆå¯¹ä¸åŒæ•°æ®åº“ç±»å‹çš„å…·ä½“å»ºè®®
            if source_type == 'postgresql' and target_type == 'clickzetta':
                suggestions.append("ğŸ’¡ PostgreSQL â†’ Clickzetta å»ºè®®:")
                suggestions.append("  â€¢ money ç±»å‹: å»ºè®®è½¬æ¢ä¸º decimal æˆ– numeric")
                suggestions.append("  â€¢ uuid ç±»å‹: å»ºè®®è½¬æ¢ä¸º string æˆ– varchar")
                suggestions.append("  â€¢ inet/macaddr ç±»å‹: å»ºè®®è½¬æ¢ä¸º string")
                suggestions.append("  â€¢ å¯è€ƒè™‘åœ¨ Clickzetta ç«¯åˆ›å»ºè§†å›¾è¿›è¡Œç±»å‹è½¬æ¢")

            suggestions.append("âš™ï¸ è§£å†³æ–¹æ¡ˆ:")
            suggestions.append("  1. å¯ç”¨ä¸¥æ ¼æ¨¡å¼æŸ¥çœ‹è¯¦ç»†ç±»å‹ä¿¡æ¯")
            suggestions.append("  2. åœ¨æ¯”å¯¹å‰é¢„å¤„ç†æ•°æ®ç±»å‹")
            suggestions.append("  3. ä½¿ç”¨ ETL å·¥å…·ç»Ÿä¸€æ•°æ®æ ¼å¼")
            suggestions.append("  4. ä»æ¯”å¯¹é…ç½®ä¸­æ’é™¤ä¸å…¼å®¹çš„åˆ—")

        if not type_warnings and not ignored_columns:
            suggestions.append("âœ… æ‰€æœ‰åˆ—ç±»å‹éƒ½å…¼å®¹ï¼Œå¯ä»¥è¿›è¡Œå®Œæ•´æ¯”å¯¹")

        return suggestions

    async def get_table_schema_preview(self,
                                     connection_config: Dict[str, Any],
                                     table_name: str) -> Dict[str, Any]:
        """
        è·å–è¡¨ç»“æ„é¢„è§ˆï¼Œå¸®åŠ©ç”¨æˆ·äº†è§£è¡¨ç»“æ„

        Args:
            connection_config: æ•°æ®åº“è¿æ¥é…ç½®
            table_name: è¡¨å

        Returns:
            è¡¨ç»“æ„ä¿¡æ¯
        """
        try:
            from data_diff import connect_to_table

            # æ„å»ºè¿æ¥å­—ç¬¦ä¸²
            if connection_config.get('database_type') == 'clickzetta':
                db_info = {
                    "driver": "clickzetta",
                    "username": connection_config.get('username'),
                    "password": connection_config.get('password'),
                    "instance": connection_config.get('instance'),
                    "service": connection_config.get('service'),
                    "workspace": connection_config.get('workspace'),
                    "schema": connection_config.get('db_schema'),
                    "virtualcluster": connection_config.get('vcluster')
                }
            else:
                conn_id = await self.connection_manager.create_connection(connection_config)
                db_info = self.connection_manager._build_connection_string(
                    self.connection_manager.get_connection_config(conn_id)
                )

            # è¿æ¥åˆ°è¡¨å¹¶è·å–schema
            try:
                table_segment = connect_to_table(
                    db_info=db_info,
                    table_name=table_name,
                    key_columns=("id",),  # ä¸´æ—¶ä½¿ç”¨ï¼Œåªæ˜¯ä¸ºäº†è·å–schema
                    thread_count=1
                ).with_schema()

                # éªŒè¯è¿æ¥æ˜¯å¦æ­£å¸¸å·¥ä½œ
                try:
                    # å°è¯•è·å–åŸºæœ¬ä¿¡æ¯æ¥éªŒè¯è¿æ¥
                    _ = table_segment.get_schema()
                    self.logger.info(f"âœ… Schema preview connection verified for table: {table_name}")
                except Exception as schema_test_error:
                    self.logger.error(f"âŒ Schema preview connection failed: {schema_test_error}")
                    raise Exception(f"æ— æ³•è·å–è¡¨ç»“æ„ï¼Œè¿æ¥å¤±è´¥æˆ–è¡¨ä¸å­˜åœ¨: {table_name} - {str(schema_test_error)}")

            except Exception as connect_error:
                self.logger.error(f"âŒ Failed to connect for schema preview: {connect_error}")
                raise Exception(f"æ— æ³•è¿æ¥åˆ°è¡¨ {table_name} è·å–ç»“æ„é¢„è§ˆ: {str(connect_error)}")

            schema_info = {}
            if hasattr(table_segment, '_schema') and table_segment._schema:
                for column, column_type in table_segment._schema.items():
                    schema_info[column] = {
                        'type': str(column_type),
                        'supported': True  # data-diff èƒ½è¯†åˆ«å°±æ˜¯æ”¯æŒçš„
                    }

            # è·å–è¡Œæ•°
            try:
                row_count = table_segment.count()
                schema_info['_row_count'] = row_count
            except Exception as e:
                schema_info['_row_count'] = f"æ— æ³•è·å–: {str(e)}"

            return {
                'status': 'success',
                'table_name': table_name,
                'schema': schema_info,
                'database_type': connection_config.get('database_type')
            }

        except Exception as e:
            return {
                'status': 'error',
                'table_name': table_name,
                'error': str(e),
                'database_type': connection_config.get('database_type')
            }
