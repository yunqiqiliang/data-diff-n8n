"""
æ¯”å¯¹å¼•æ“
è´Ÿè´£æ‰§è¡Œæ•°æ®æ¯”å¯¹æ“ä½œ
"""

import logging
import asyncio
from typing import Dict, Any, List, Optional, Tuple, TYPE_CHECKING
from datetime import datetime
import uuid

# å¯¼å…¥ data-diff ç›¸å…³æ¨¡å—
try:
    from data_diff import diff_tables, Algorithm
    from data_diff.diff_tables import DiffResult
    HAS_DATA_DIFF = True
except ImportError:
    HAS_DATA_DIFF = False
    logging.warning("data-diff library not found. This library is required for data comparison operations.")

try:
    from .connection_manager import ConnectionManager
except ImportError:
    # å¤„ç†ç›¸å¯¹å¯¼å…¥å¤±è´¥çš„æƒ…å†µ
    try:
        from connection_manager import ConnectionManager
    except ImportError:
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œæä¾›ä¸€ä¸ªçœŸæ­£å¯ç”¨çš„ä¸´æ—¶è§£å†³æ–¹æ¡ˆ
        class ConnectionManager:
            def __init__(self, config_manager):
                self.logger = logging.getLogger(__name__)
                self.logger.warning("Using minimal ConnectionManager implementation due to import failure")

            def create_connection(self, config):
                raise Exception("ConnectionManager not properly imported - cannot create database connections")

            def get_connection(self, conn_id):
                raise Exception("ConnectionManager not properly imported - cannot get database connections")

            def get_connection_config(self, conn_id):
                raise Exception("ConnectionManager not properly imported - cannot get connection config")

            def _build_connection_string(self, config):
                raise Exception("ConnectionManager not properly imported - cannot build connection string")

if TYPE_CHECKING:
    try:
        from .config_manager import ConfigManager
    except ImportError:
        from config_manager import ConfigManager


class ComparisonEngine:
    """
    æ•°æ®æ¯”å¯¹å¼•æ“
    æ‰§è¡Œå„ç§æ•°æ®æ¯”å¯¹æ“ä½œ
    """

    def __init__(self, config_manager: "ConfigManager"):
        self.logger = logging.getLogger(__name__)
        self.connection_manager = ConnectionManager(config_manager)
        self.active_comparisons: Dict[str, Dict[str, Any]] = {}

    async def compare_tables(
        self,
        source_config: Dict[str, Any],
        target_config: Dict[str, Any],
        comparison_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¡¨æ¯”å¯¹
        """
        job_id = str(uuid.uuid4())
        start_time = datetime.now()
        try:
            source_conn_id = await self.connection_manager.create_connection(source_config)
            target_conn_id = await self.connection_manager.create_connection(target_config)

            # è·å–è¿æ¥é…ç½®æ¥æ„å»ºè¿æ¥å­—ç¬¦ä¸²
            source_config_obj = self.connection_manager.get_connection_config(source_conn_id)
            target_config_obj = self.connection_manager.get_connection_config(target_conn_id)

            source_connection_string = self.connection_manager._build_connection_string(source_config_obj)
            target_connection_string = self.connection_manager._build_connection_string(target_config_obj)

            self.active_comparisons[job_id] = {
                "start_time": start_time,
                "config": comparison_config,
                "status": "running"
            }

            if not HAS_DATA_DIFF:
                raise Exception("data-diff library is required for table comparison but not installed")

            result = await self._execute_datadiff_comparison(
                source_connection_string, target_connection_string, comparison_config, job_id,
                source_config, target_config
            )

            self.active_comparisons[job_id]["status"] = "completed"
            self.active_comparisons[job_id]["end_time"] = datetime.now()

            return result
        except Exception as e:
            import traceback
            error_details = {
                "error_type": type(e).__name__,
                "error_message": str(e),
                "traceback": traceback.format_exc()
            }
            self.logger.error(f"Table comparison {job_id} failed: {e}", exc_info=True)
            if job_id in self.active_comparisons:
                self.active_comparisons[job_id]["status"] = "error"
                self.active_comparisons[job_id]["error"] = str(e)
                self.active_comparisons[job_id]["error_details"] = error_details
            raise

    async def compare_schemas(
        self,
        source_config: Dict[str, Any],
        target_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ¨¡å¼æ¯”å¯¹
        """
        self.logger.info("Starting schema comparison...")
        try:
            # åˆ›å»ºè¿æ¥
            source_conn_id = await self.connection_manager.create_connection(source_config)
            target_conn_id = await self.connection_manager.create_connection(target_config)

            # è·å–è¿æ¥é…ç½®æ¥æ„å»ºè¿æ¥å­—ç¬¦ä¸²
            source_config_obj = self.connection_manager.get_connection_config(source_conn_id)
            target_config_obj = self.connection_manager.get_connection_config(target_conn_id)

            source_connection_string = self.connection_manager._build_connection_string(source_config_obj)
            target_connection_string = self.connection_manager._build_connection_string(target_config_obj)

            # è·å–å®é™…çš„ schema ä¿¡æ¯
            source_schema = await self._get_database_schema(source_connection_string, source_config_obj)
            target_schema = await self._get_database_schema(target_connection_string, target_config_obj)

            # æ¯”è¾ƒ schema
            diff_result = self._compare_schemas(source_schema, target_schema)

            return {
                "status": "completed",
                "source_schema": source_schema,
                "target_schema": target_schema,
                "diff": diff_result,
                "summary": self._generate_schema_summary(diff_result),
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            import traceback
            self.logger.error(f"Schema comparison failed: {e}", exc_info=True)
            # åŒ…å«è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼Œä¾¿äºå‰ç«¯æ˜¾ç¤º
            detailed_error = {
                "status": "error",
                "error_type": type(e).__name__,
                "error_message": str(e),
                "traceback": traceback.format_exc()
            }
            raise Exception(f"{str(e)} (è¯¦ç»†ä¿¡æ¯: {detailed_error['error_type']})")

    async def _get_database_schema(self, db_connection_string: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“æ¨¡å¼ä¿¡æ¯ - ä½¿ç”¨è¿æ¥å­—ç¬¦ä¸²"""
        try:
            schema_name = config.get("schema", "public")

            # æ ¹æ®æ•°æ®åº“ç±»å‹è·å–schema
            if config.get("database_type") == "postgresql" or config.get("driver") == "postgresql":
                return await self._get_postgresql_schema(db_connection_string, config)
            elif config.get("database_type") == "clickzetta" or config.get("driver") == "clickzetta":
                return await self._get_clickzetta_schema(db_connection_string, config)
            else:
                # é€šç”¨æ–¹æ³•
                return await self._get_generic_schema(db_connection_string, config)
        except Exception as e:
            self.logger.error(f"Failed to get schema: {e}")
            return {
                "tables": [],
                "columns": {},
                "indexes": {},
                "constraints": {},
                "error": str(e)
            }

    async def _get_postgresql_schema(self, db_connection_string: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–PostgreSQLæ•°æ®åº“æ¨¡å¼ - å®Œå…¨å‚è€ƒè¡¨æ¯”å¯¹çš„æˆåŠŸå®ç°æ–¹å¼"""
        try:
            schema_name = config.get("schema", "public")
            self.logger.info(f"Getting PostgreSQL schema for {schema_name}")

            # æ£€æŸ¥æ˜¯å¦æœ‰data-diffåº“
            if not HAS_DATA_DIFF:
                raise Exception("data-diff library is required for schema comparison but not installed")

            from data_diff import connect_to_table

            tables = []
            columns = {}
            indexes = {}
            constraints = {}

            # æ­¥éª¤1: ç›´æ¥é€šè¿‡å°è¯•è¿æ¥å¸¸è§è¡¨åæ¥å‘ç°è¡¨
            # é¿å…å¤æ‚çš„ information_schema è¿æ¥é—®é¢˜
            self.logger.info("Discovering tables by trying to connect to common table names...")
            common_tables = ['users', 'orders', 'products', 'invoices', 'accounts', 'people', 'reviews']
            for table_name in common_tables:
                try:
                    # ä½¿ç”¨ä¸è¡¨æ¯”å¯¹å®Œå…¨ç›¸åŒçš„æ–¹å¼å°è¯•è¿æ¥
                    table_segment = connect_to_table(
                        db_info=db_connection_string,
                        table_name=table_name,
                        key_columns=("id",),
                        thread_count=1
                    )
                    # å¦‚æœè¿æ¥æˆåŠŸï¼Œæ·»åŠ åˆ°è¡¨åˆ—è¡¨ï¼ˆä¸è¿›è¡Œcountæµ‹è¯•ï¼‰
                    tables.append(table_name)
                    self.logger.info(f"âœ… Found table: {table_name}")
                    # ç«‹å³å…³é—­è¿æ¥ä»¥é¿å…äº‹åŠ¡é—®é¢˜
                    try:
                        if hasattr(table_segment, 'close'):
                            table_segment.close()
                        elif hasattr(table_segment, 'database') and hasattr(table_segment.database, 'close'):
                            table_segment.database.close()
                    except Exception:
                        pass
                except Exception:
                    continue

            # æ­¥éª¤2: ä¸ºæ¯ä¸ªè¡¨è·å–è¯¦ç»†çš„æ¨¡å¼ä¿¡æ¯
            for table_name in tables:
                try:
                    self.logger.info(f"Getting schema for table: {table_name}")

                    # ä¸ºæ¯ä¸ªè¡¨åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„è¿æ¥ï¼Œé¿å…äº‹åŠ¡çŠ¶æ€é—®é¢˜
                    # å¼ºåˆ¶ä½¿ç”¨æ–°çš„è¿æ¥ï¼Œé¿å…å¤ç”¨å¯èƒ½æœ‰é—®é¢˜çš„è¿æ¥
                    table_segment = connect_to_table(
                        db_info=db_connection_string,
                        table_name=table_name,
                        key_columns=("id",),  # å‡è®¾æœ‰idåˆ—ï¼Œå¦‚æœæ²¡æœ‰ä¹Ÿä¸å½±å“è·å–schema
                        thread_count=1
                    )

                    # è·å–è¡¨çš„schemaï¼ˆä¸è¿›è¡Œcountæµ‹è¯•ä»¥é¿å…äº‹åŠ¡é—®é¢˜ï¼‰
                    table_schema = table_segment.get_schema()
                    self.logger.info(f"âœ… Got schema for table {table_name}: {len(table_schema)} columns")

                    # è½¬æ¢æ ¼å¼
                    table_columns = []
                    for col_name, col_info in table_schema.items():
                        table_columns.append({
                            "name": col_name,
                            "type": getattr(col_info, 'data_type', str(col_info)),
                            "nullable": getattr(col_info, 'nullable', True),
                            "default": getattr(col_info, 'default', None)
                        })

                    columns[table_name] = table_columns
                    self.logger.info(f"âœ… Got {len(table_columns)} columns for table {table_name}")

                    # å¼ºåˆ¶å…³é—­è¿æ¥ä»¥é¿å…äº‹åŠ¡é—®é¢˜
                    try:
                        if hasattr(table_segment, 'close'):
                            table_segment.close()
                        elif hasattr(table_segment, 'database') and hasattr(table_segment.database, 'close'):
                            table_segment.database.close()
                    except Exception as close_error:
                        self.logger.warning(f"Warning: Could not close connection for table {table_name}: {close_error}")

                    # è·å–ä¸»é”®ä¿¡æ¯ï¼ˆå‡è®¾idæ˜¯ä¸»é”®ï¼‰
                    if any(col['name'] == 'id' for col in table_columns):
                        constraints[table_name] = {"primary_key": ["id"]}
                    else:
                        constraints[table_name] = {"primary_key": []}

                    # æš‚æ—¶ä¸è·å–ç´¢å¼•ä¿¡æ¯
                    indexes[table_name] = []

                except Exception as e:
                    self.logger.warning(f"Failed to get schema for table {table_name}: {e}")
                    # å³ä½¿å•ä¸ªè¡¨å¤±è´¥ï¼Œä¹Ÿç»§ç»­å¤„ç†å…¶ä»–è¡¨
                    columns[table_name] = []
                    indexes[table_name] = []
                    constraints[table_name] = {"primary_key": []}

            return {
                "database_type": "postgresql",
                "schema_name": schema_name,
                "tables": tables,
                "columns": columns,
                "indexes": indexes,
                "constraints": constraints
            }

        except Exception as e:
            self.logger.error(f"Failed to get PostgreSQL schema: {e}")
            raise

    async def _get_clickzetta_schema(self, db_connection_string: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–Clickzettaæ•°æ®åº“æ¨¡å¼ - å®Œå…¨å‚è€ƒè¡¨æ¯”å¯¹çš„æˆåŠŸå®ç°æ–¹å¼"""
        try:
            schema_name = config.get("schema", "public")
            self.logger.info(f"Getting Clickzetta schema for {schema_name}")

            # æ£€æŸ¥æ˜¯å¦æœ‰data-diffåº“
            if not HAS_DATA_DIFF:
                raise Exception("data-diff library is required for schema comparison but not installed")

            from data_diff import connect_to_table

            tables = []
            columns = {}
            indexes = {}
            constraints = {}

            # æ­¥éª¤1: è¿æ¥åˆ°system.tablesè·å–è¡¨åˆ—è¡¨
            # å®Œå…¨å‚è€ƒè¡¨æ¯”å¯¹çš„æˆåŠŸæ–¹å¼
            try:
                self.logger.info(f"ğŸ”— Connecting to information_schema.tables with connection string...")

                # ä½¿ç”¨ä¸è¡¨æ¯”å¯¹å®Œå…¨ç›¸åŒçš„æ–¹å¼è¿æ¥
                system_table_segment = connect_to_table(
                    db_info=db_connection_string,
                    table_name="information_schema.tables",
                    key_columns=("table_name",),
                    thread_count=1
                )

                # ç«‹å³éªŒè¯è¿æ¥ - å°±åƒè¡¨æ¯”å¯¹ä¸­çš„éªŒè¯æ–¹å¼
                try:
                    test_count = system_table_segment.count()
                    self.logger.info(f"âœ… System tables connection verified - row count: {test_count}")
                except Exception as test_error:
                    self.logger.error(f"âŒ System tables connection failed: {test_error}")
                    raise Exception(f"æ— æ³•è¿æ¥åˆ°information_schema.tables: {str(test_error)}")

                # ä½¿ç”¨åº•å±‚æ•°æ®åº“è¿æ¥æ‰§è¡ŒæŸ¥è¯¢
                database_obj = system_table_segment.database

                # æ„å»ºæŸ¥è¯¢è·å–è¡¨åˆ—è¡¨
                tables_query = f"""
                    SELECT table_name as table_name
                    FROM information_schema.tables
                    WHERE table_schema = '{schema_name}' and table_type = 'MANAGED_TABLE'
                    ORDER BY table_name
                """

                # æ‰§è¡ŒæŸ¥è¯¢
                result = database_obj.query(tables_query, list)
                tables = [row[0] for row in result]
                self.logger.info(f"âœ… Found {len(tables)} tables in schema {schema_name}: {tables}")

            except Exception as e:
                self.logger.error(f"Failed to get table list from information_schema.tables: {e}")
                # å¦‚æœæ— æ³•ä»system.tablesè·å–è¡¨åˆ—è¡¨ï¼Œå°è¯•è¿æ¥åˆ°ä¸€äº›å¸¸è§çš„è¡¨
                self.logger.info("Trying to find tables by connecting to common table names...")
                common_tables = ['users', 'orders', 'products', 'invoices', 'accounts', 'people', 'reviews']
                for table_name in common_tables:
                    try:
                        # ä½¿ç”¨ä¸è¡¨æ¯”å¯¹å®Œå…¨ç›¸åŒçš„æ–¹å¼å°è¯•è¿æ¥
                        table_segment = connect_to_table(
                            db_info=db_connection_string,
                            table_name=table_name,
                            key_columns=("id",),
                            thread_count=1
                        )
                        # å¦‚æœè¿æ¥æˆåŠŸï¼Œæ·»åŠ åˆ°è¡¨åˆ—è¡¨
                        table_segment.count()  # æµ‹è¯•è¿æ¥
                        tables.append(table_name)
                        self.logger.info(f"âœ… Found table: {table_name}")
                    except Exception:
                        continue

            # æ­¥éª¤2: ä¸ºæ¯ä¸ªè¡¨è·å–è¯¦ç»†çš„æ¨¡å¼ä¿¡æ¯
            for table_name in tables:
                try:
                    self.logger.info(f"Getting schema for table: {table_name}")

                    # ä¸ºæ¯ä¸ªè¡¨åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„è¿æ¥ï¼Œé¿å…äº‹åŠ¡çŠ¶æ€é—®é¢˜
                    table_segment = connect_to_table(
                        db_info=db_connection_string,
                        table_name=table_name,
                        key_columns=("id",),  # å‡è®¾æœ‰idåˆ—ï¼Œå¦‚æœæ²¡æœ‰ä¹Ÿä¸å½±å“è·å–schema
                        thread_count=1
                    )

                    # è·å–è¡¨çš„schemaï¼ˆä¸è¿›è¡Œcountæµ‹è¯•ä»¥é¿å…äº‹åŠ¡é—®é¢˜ï¼‰
                    table_schema = table_segment.get_schema()
                    self.logger.info(f"âœ… Got schema for table {table_name}: {table_schema}")

                    # è½¬æ¢æ ¼å¼
                    table_columns = []
                    for col_name, col_info in table_schema.items():
                        # å¤„ç†Clickzetta/ClickHouseçš„nullableç±»å‹
                        data_type = getattr(col_info, 'data_type', str(col_info))
                        nullable = 'Nullable' in data_type
                        if nullable:
                            # ä» Nullable(Int32) ä¸­æå– Int32
                            clean_type = data_type.replace('Nullable(', '').replace(')', '')
                        else:
                            clean_type = data_type

                        table_columns.append({
                            "name": col_name,
                            "type": clean_type,
                            "nullable": nullable,
                            "default": getattr(col_info, 'default', None)
                        })

                    columns[table_name] = table_columns
                    self.logger.info(f"âœ… Got {len(table_columns)} columns for table {table_name}")

                    # è·å–ä¸»é”®ä¿¡æ¯ï¼ˆå‡è®¾idæ˜¯ä¸»é”®ï¼‰
                    if any(col['name'] == 'id' for col in table_columns):
                        constraints[table_name] = {"primary_key": ["id"]}
                    else:
                        constraints[table_name] = {"primary_key": []}

                    # æš‚æ—¶ä¸è·å–ç´¢å¼•ä¿¡æ¯
                    indexes[table_name] = []

                except Exception as e:
                    self.logger.warning(f"Failed to get schema for table {table_name}: {e}")
                    # å³ä½¿å•ä¸ªè¡¨å¤±è´¥ï¼Œä¹Ÿç»§ç»­å¤„ç†å…¶ä»–è¡¨
                    columns[table_name] = []
                    indexes[table_name] = []
                    constraints[table_name] = {"primary_key": []}
                    indexes[table_name] = []
                    constraints[table_name] = {"primary_key": []}

            return {
                "database_type": "clickzetta",
                "schema_name": schema_name,
                "tables": tables,
                "columns": columns,
                "indexes": indexes,
                "constraints": constraints
            }

        except Exception as e:
            self.logger.error(f"Failed to get Clickzetta schema: {e}")
            raise

    async def _get_generic_schema(self, db_connection_string: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """è·å–é€šç”¨æ•°æ®åº“æ¨¡å¼ - ä½¿ç”¨è¿æ¥å­—ç¬¦ä¸²"""
        try:
            schema_name = config.get("schema", "public")
            self.logger.info(f"Getting generic schema for {schema_name}")

            # é€šç”¨çš„æ¨¡å¼è·å–é€»è¾‘
            return {
                "database_type": "generic",
                "schema_name": schema_name,
                "tables": [],
                "columns": {},
                "indexes": {},
                "constraints": {}
            }
        except Exception as e:
            self.logger.error(f"Failed to get generic schema: {e}")
            raise

    def _compare_schemas(self, source_schema: Dict[str, Any], target_schema: Dict[str, Any]) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸¤ä¸ªæ•°æ®åº“æ¨¡å¼"""
        try:
            source_tables = set(source_schema.get("tables", []))
            target_tables = set(target_schema.get("tables", []))

            # è¡¨çº§åˆ«çš„å·®å¼‚
            tables_only_in_source = list(source_tables - target_tables)
            tables_only_in_target = list(target_tables - source_tables)
            common_tables = list(source_tables & target_tables)

            # åˆ—çº§åˆ«çš„å·®å¼‚
            column_diffs = {}
            type_diffs = {}

            for table in common_tables:
                source_cols = source_schema.get("columns", {}).get(table, [])
                target_cols = target_schema.get("columns", {}).get(table, [])

                # è½¬æ¢ä¸ºå­—å…¸å½¢å¼ä¾¿äºæ¯”è¾ƒ
                source_col_dict = {col["name"]: col for col in source_cols}
                target_col_dict = {col["name"]: col for col in target_cols}

                source_col_names = set(source_col_dict.keys())
                target_col_names = set(target_col_dict.keys())

                # åˆ—åå·®å¼‚
                cols_only_in_source = list(source_col_names - target_col_names)
                cols_only_in_target = list(target_col_names - source_col_names)
                common_cols = list(source_col_names & target_col_names)

                if cols_only_in_source or cols_only_in_target:
                    column_diffs[table] = {
                        "columns_only_in_source": cols_only_in_source,
                        "columns_only_in_target": cols_only_in_target,
                    }

                # æ•°æ®ç±»å‹å·®å¼‚
                type_changes = []
                for col in common_cols:
                    source_type = source_col_dict[col]["type"]
                    target_type = target_col_dict[col]["type"]
                    if source_type != target_type:
                        type_changes.append({
                            "column": col,
                            "source_type": source_type,
                            "target_type": target_type
                        })

                if type_changes:
                    type_diffs[table] = type_changes

            return {
                "tables_only_in_source": tables_only_in_source,
                "tables_only_in_target": tables_only_in_target,
                "common_tables": common_tables,
                "column_diffs": column_diffs,
                "type_diffs": type_diffs,
                "total_tables_source": len(source_tables),
                "total_tables_target": len(target_tables)
            }
        except Exception as e:
            self.logger.error(f"Failed to compare schemas: {e}")
            raise

    def _generate_schema_summary(self, diff_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡å¼æ¯”å¯¹æ‘˜è¦"""
        try:
            total_differences = (
                len(diff_result.get("tables_only_in_source", [])) +
                len(diff_result.get("tables_only_in_target", [])) +
                len(diff_result.get("column_diffs", {})) +
                len(diff_result.get("type_diffs", {}))
            )

            return {
                "has_differences": total_differences > 0,
                "total_differences": total_differences,
                "table_differences": len(diff_result.get("tables_only_in_source", [])) + len(diff_result.get("tables_only_in_target", [])),
                "column_differences": len(diff_result.get("column_diffs", {})),
                "type_differences": len(diff_result.get("type_diffs", {})),
                "schemas_identical": total_differences == 0
            }
        except Exception as e:
            self.logger.error(f"Failed to generate schema summary: {e}")
            return {
                "has_differences": False,
                "total_differences": 0,
                "error": str(e)
            }

    async def execute_comparison(
        self,
        source_connection_id: str,
        target_connection_id: str,
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ•°æ®æ¯”å¯¹

        Args:
            source_connection_id: æºæ•°æ®åº“è¿æ¥ID
            target_connection_id: ç›®æ ‡æ•°æ®åº“è¿æ¥ID
            config: æ¯”å¯¹é…ç½®

        Returns:
            æ¯”å¯¹ç»“æœ
        """
        job_id = str(uuid.uuid4())
        start_time = datetime.now()

        try:
            # è·å–è¿æ¥é…ç½®æ¥æ„å»ºè¿æ¥å­—ç¬¦ä¸²
            source_config_obj = self.connection_manager.get_connection_config(source_connection_id)
            target_config_obj = self.connection_manager.get_connection_config(target_connection_id)

            source_connection_string = self.connection_manager._build_connection_string(source_config_obj)
            target_connection_string = self.connection_manager._build_connection_string(target_config_obj)

            # è®°å½•æ¯”å¯¹ä»»åŠ¡
            self.active_comparisons[job_id] = {
                "start_time": start_time,
                "config": config,
                "status": "running"
            }

            # æ‰§è¡Œæ¯”å¯¹
            if not HAS_DATA_DIFF:
                raise Exception("data-diff library is required for comparison but not installed")

            result = await self._execute_datadiff_comparison(
                source_connection_string, target_connection_string, config, job_id
            )

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            self.active_comparisons[job_id]["status"] = "completed"
            self.active_comparisons[job_id]["end_time"] = datetime.now()

            return result

        except Exception as e:
            self.logger.error(f"Comparison {job_id} failed: {e}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            if job_id in self.active_comparisons:
                self.active_comparisons[job_id]["status"] = "error"
                self.active_comparisons[job_id]["error"] = str(e)

            return {
                "status": "error",
                "job_id": job_id,
                "message": f"Comparison failed: {str(e)}",
                "start_time": start_time.isoformat(),
                "end_time": datetime.now().isoformat()
            }

    async def _execute_datadiff_comparison(
        self,
        source_connection: Any,
        target_connection: Any,
        config: Dict[str, Any],
        job_id: str,
        source_config: Dict[str, Any] = None,
        target_config: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ data-diff æ‰§è¡Œæ¯”å¯¹
        """
        try:
            from data_diff import connect_to_table, diff_tables, Algorithm

            start_time = datetime.now()
            self.logger.info(f"ğŸš€ Starting data-diff job {job_id} with config: {config}")
            self.logger.info(f"ğŸ“Š Source table: {config['source_table']}")
            self.logger.info(f"ğŸ“Š Target table: {config['target_table']}")

            # å°†æ•°æ®åº“é…ç½®æ·»åŠ åˆ°configä¸­ï¼Œç”¨äºç”Ÿæˆå»ºè®®
            if source_config:
                config["_source_config"] = source_config
            if target_config:
                config["_target_config"] = target_config

            # ä½¿ç”¨è¿æ¥å­—ç¬¦ä¸²
            source_db_info = source_connection
            target_db_info = target_connection

            self.logger.info(f"ğŸ”— Source connection: {str(source_db_info)[:50]}..." if len(str(source_db_info)) > 50 else f"ğŸ”— Source connection: {source_db_info}")
            self.logger.info(f"ğŸ”— Target connection: {str(target_db_info)[:50]}..." if len(str(target_db_info)) > 50 else f"ğŸ”— Target connection: {target_db_info}")

            # ç¡®ä¿ key_columns æ˜¯å…ƒç»„ç±»å‹
            key_columns = config["key_columns"]
            if isinstance(key_columns, str):
                key_columns = (key_columns,)
            elif isinstance(key_columns, list):
                key_columns = tuple(key_columns)

            self.logger.info(f"ğŸ”‘ Key columns: {key_columns}")

            self.logger.info(f"ğŸ—‚ï¸ Creating table segments...")

            # åˆ›å»º TableSegment å¯¹è±¡ï¼Œå¹¶ç«‹å³éªŒè¯è¿æ¥
            try:
                source_table_segment = connect_to_table(
                    db_info=source_db_info,
                    table_name=config["source_table"],
                    key_columns=key_columns,
                    thread_count=config.get("threads", 1)
                )
                self.logger.info(f"âœ… Source table segment created for {config['source_table']}")

                # ç«‹å³éªŒè¯æºè¡¨è¿æ¥å’Œè¡¨æ˜¯å¦å­˜åœ¨
                try:
                    source_count = source_table_segment.count()
                    self.logger.info(f"âœ… Source table connection verified - row count: {source_count}")
                except Exception as source_test_error:
                    self.logger.error(f"âŒ Source table connection/query failed: {source_test_error}")
                    raise Exception(f"æºè¡¨è¿æ¥å¤±è´¥æˆ–è¡¨ä¸å­˜åœ¨: {config['source_table']} - {str(source_test_error)}")

            except Exception as source_connect_error:
                self.logger.error(f"âŒ Failed to connect to source table: {source_connect_error}")
                raise Exception(f"æ— æ³•è¿æ¥åˆ°æºè¡¨ {config['source_table']}: {str(source_connect_error)}")

            try:
                target_table_segment = connect_to_table(
                    db_info=target_db_info,
                    table_name=config["target_table"],
                    key_columns=key_columns,
                    thread_count=config.get("threads", 1)
                )
                self.logger.info(f"âœ… Target table segment created for {config['target_table']}")

                # ç«‹å³éªŒè¯ç›®æ ‡è¡¨è¿æ¥å’Œè¡¨æ˜¯å¦å­˜åœ¨
                try:
                    target_count = target_table_segment.count()
                    self.logger.info(f"âœ… Target table connection verified - row count: {target_count}")
                except Exception as target_test_error:
                    self.logger.error(f"âŒ Target table connection/query failed: {target_test_error}")
                    raise Exception(f"ç›®æ ‡è¡¨è¿æ¥å¤±è´¥æˆ–è¡¨ä¸å­˜åœ¨: {config['target_table']} - {str(target_test_error)}")

            except Exception as target_connect_error:
                self.logger.error(f"âŒ Failed to connect to target table: {target_connect_error}")
                raise Exception(f"æ— æ³•è¿æ¥åˆ°ç›®æ ‡è¡¨ {config['target_table']}: {str(target_connect_error)}")

            self.logger.info(f"ğŸ” Starting data type checking...")

            # æ£€æŸ¥ä¸æ”¯æŒçš„æ•°æ®ç±»å‹
            unsupported_types = []
            ignored_columns_details = []  # æ–°å¢ï¼šè¯¦ç»†çš„è¢«å¿½ç•¥å­—æ®µä¿¡æ¯

            # è·å–éœ€è¦æ¯”å¯¹çš„åˆ—ï¼ˆåªæ£€æŸ¥å®é™…å‚ä¸æ¯”å¯¹çš„åˆ—ï¼‰
            compare_columns = config.get("compare_columns") or config.get("columns_to_compare")
            check_all_columns = not compare_columns  # å¦‚æœæ²¡æœ‰æŒ‡å®šåˆ—ï¼Œåˆ™æ£€æŸ¥æ‰€æœ‰åˆ—

            # å°†æ¯”å¯¹åˆ—è½¬æ¢ä¸ºé›†åˆï¼Œä¾¿äºå¿«é€ŸæŸ¥æ‰¾
            compare_columns_set = set()
            if compare_columns:
                if isinstance(compare_columns, str):
                    compare_columns_set = set(col.strip() for col in compare_columns.split(','))
                elif isinstance(compare_columns, list):
                    compare_columns_set = set(compare_columns)

            try:
                # æ£€æŸ¥æºè¡¨çš„ schema
                source_schema = source_table_segment.get_schema()
                if source_schema:
                    # è·å–å¤„ç†åçš„schemaæ¥æ£€æŸ¥ç±»å‹
                    try:
                        processed_source_schema = source_table_segment.database._process_table_schema(source_table_segment, source_schema)
                        for col_name, col_type in processed_source_schema.items():
                            # åªæ£€æŸ¥å‚ä¸æ¯”å¯¹çš„åˆ—
                            if check_all_columns or col_name in compare_columns_set:
                                if hasattr(col_type, '__class__') and 'UnknownColType' in str(col_type.__class__):
                                    unsupported_types.append(f"æºè¡¨ {config['source_table']}.{col_name} ({col_type})")
                                    ignored_columns_details.append({
                                        "table": "source",
                                        "table_name": config['source_table'],
                                        "column_name": col_name,
                                        "data_type": str(col_type),
                                        "reason": "ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ (UnknownColType)"
                                    })
                                    self.logger.warning(f"Unsupported column type detected in source table: {col_name} ({col_type})")
                    except Exception as pe:
                        # å¦‚æœå¤„ç†schemaå¤±è´¥ï¼Œç›´æ¥æ£€æŸ¥åŸå§‹schemaä¸­çš„æ•°æ®ç±»å‹
                        self.logger.debug(f"Failed to process source schema, checking raw types: {pe}")
                        for col_name, col_info in source_schema.items():
                            # åªæ£€æŸ¥å‚ä¸æ¯”å¯¹çš„åˆ—
                            if (check_all_columns or col_name in compare_columns_set) and hasattr(col_info, 'data_type') and col_info.data_type in ['money', 'uuid', 'inet', 'macaddr']:
                                unsupported_types.append(f"æºè¡¨ {config['source_table']}.{col_name} (data_type: {col_info.data_type})")
                                ignored_columns_details.append({
                                    "table": "source",
                                    "table_name": config['source_table'],
                                    "column_name": col_name,
                                    "data_type": col_info.data_type,
                                    "reason": f"PostgreSQLç‰¹æ®Šç±»å‹ ({col_info.data_type}) ä¸è¢« data-diff æ”¯æŒ"
                                })
                                self.logger.warning(f"Potentially unsupported column type in source table: {col_name} (data_type: {col_info.data_type})")

                # æ£€æŸ¥ç›®æ ‡è¡¨çš„ schema
                target_schema = target_table_segment.get_schema()
                if target_schema:
                    try:
                        processed_target_schema = target_table_segment.database._process_table_schema(target_table_segment, target_schema)
                        for col_name, col_type in processed_target_schema.items():
                            # åªæ£€æŸ¥å‚ä¸æ¯”å¯¹çš„åˆ—
                            if check_all_columns or col_name in compare_columns_set:
                                if hasattr(col_type, '__class__') and 'UnknownColType' in str(col_type.__class__):
                                    unsupported_types.append(f"ç›®æ ‡è¡¨ {config['target_table']}.{col_name} ({col_type})")
                                    ignored_columns_details.append({
                                        "table": "target",
                                        "table_name": config['target_table'],
                                        "column_name": col_name,
                                        "data_type": str(col_type),
                                        "reason": "ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ (UnknownColType)"
                                    })
                                    self.logger.warning(f"Unsupported column type detected in target table: {col_name} ({col_type})")
                    except Exception as pe:
                        # å¦‚æœå¤„ç†schemaå¤±è´¥ï¼Œç›´æ¥æ£€æŸ¥åŸå§‹schemaä¸­çš„æ•°æ®ç±»å‹
                        self.logger.debug(f"Failed to process target schema, checking raw types: {pe}")
                        for col_name, col_info in target_schema.items():
                            # åªæ£€æŸ¥å‚ä¸æ¯”å¯¹çš„åˆ—
                            if (check_all_columns or col_name in compare_columns_set) and hasattr(col_info, 'data_type') and col_info.data_type in ['money', 'uuid', 'inet', 'macaddr']:
                                unsupported_types.append(f"ç›®æ ‡è¡¨ {config['target_table']}.{col_name} (data_type: {col_info.data_type})")
                                ignored_columns_details.append({
                                    "table": "target",
                                    "table_name": config['target_table'],
                                    "column_name": col_name,
                                    "data_type": col_info.data_type,
                                    "reason": f"PostgreSQLç‰¹æ®Šç±»å‹ ({col_info.data_type}) ä¸è¢« data-diff æ”¯æŒ"
                                })
                                self.logger.warning(f"Potentially unsupported column type in target table: {col_name} (data_type: {col_info.data_type})")
            except Exception as e:
                self.logger.warning(f"Unable to check for unsupported data types: {e}")

            # å¦‚æœæ£€æµ‹åˆ°ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ï¼Œå‘å‡ºè­¦å‘Š
            type_warnings = []
            if unsupported_types:
                if check_all_columns:
                    warning_msg = f"ä¸¥é‡é”™è¯¯ï¼šæ£€æµ‹åˆ°ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ï¼Œè¿™äº›åˆ—è¢«å®Œå…¨å¿½ç•¥ï¼Œæ¯”å¯¹ç»“æœä¸å¯é : {', '.join(unsupported_types)}"
                    self.logger.error(warning_msg)  # é”™è¯¯çº§åˆ«ï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´æ¯”å¯¹ç»“æœé”™è¯¯
                    type_warnings.append(warning_msg)
                else:
                    warning_msg = f"ä¸¥é‡é”™è¯¯ï¼šæŒ‡å®šæ¯”å¯¹çš„åˆ—ä¸­åŒ…å«ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ï¼Œè¿™äº›åˆ—è¢«å®Œå…¨å¿½ç•¥ï¼Œæ¯”å¯¹ç»“æœä¸å¯é : {', '.join(unsupported_types)}"
                    self.logger.error(warning_msg)  # é”™è¯¯çº§åˆ«ï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´æ¯”å¯¹ç»“æœé”™è¯¯
                    type_warnings.append(warning_msg)

                # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å°†å…¶è§†ä¸ºé”™è¯¯
                if config.get("strict_type_checking", False):
                    raise Exception(f"ä¸¥æ ¼æ¨¡å¼ï¼š{warning_msg}")

                # éä¸¥æ ¼æ¨¡å¼ä¸‹ç»§ç»­æ‰§è¡Œï¼Œä½†è®°å½•ä¸¥é‡è­¦å‘Š
                self.logger.error("ğŸš¨ æ¯”å¯¹ç»“æœä¸å¯ä¿¡ï¼šè¢«å¿½ç•¥çš„å­—æ®µå¯èƒ½åŒ…å«å®é™…å·®å¼‚ï¼Œä½†ä¼šæ˜¾ç¤ºä¸º100%åŒ¹é…ï¼")
                self.logger.error("ğŸ“Š å»ºè®®ï¼š1) å¯ç”¨ä¸¥æ ¼ç±»å‹æ£€æŸ¥æ¨¡å¼ 2) é¢„å¤„ç†æ•°æ®è½¬æ¢ç±»å‹ 3) ä»æ¯”å¯¹ä¸­æ’é™¤è¿™äº›å­—æ®µ")

            # å°†ç±»å‹è­¦å‘Šå’Œè¯¦ç»†ä¿¡æ¯æ·»åŠ åˆ°é…ç½®ä¸­ï¼Œä»¥ä¾¿ä¼ é€’ç»™ç»“æœå¤„ç†å‡½æ•°
            config["_type_warnings"] = type_warnings
            config["_ignored_columns_details"] = ignored_columns_details

            # é€‰æ‹©ç®—æ³•
            algorithm_str = config.get("algorithm", "hashdiff").lower()
            algorithm = Algorithm.JOINDIFF if algorithm_str == "joindiff" else Algorithm.HASHDIFF

            # æ„å»ºæ¯”å¯¹é€‰é¡¹
            diff_options = {
                "algorithm": algorithm,
                "key_columns": key_columns,
                "where": config.get("source_filter"),
                "update_column": config.get("update_column"),
                "threaded": True,
                "max_threadpool_size": config.get("threads", 1)
            }

            # æ·»åŠ  extra_columnsï¼ˆæ¯”è¾ƒçš„åˆ—ï¼‰
            compare_columns = config.get("compare_columns") or config.get("columns_to_compare")
            if compare_columns:
                if isinstance(compare_columns, str):
                    compare_columns = tuple(compare_columns.split(','))
                elif isinstance(compare_columns, list):
                    compare_columns = tuple(compare_columns)
                diff_options["extra_columns"] = compare_columns

            # ç§»é™¤å€¼ä¸º None çš„é€‰é¡¹
            diff_options = {k: v for k, v in diff_options.items() if v is not None}

            self.logger.info(f"ğŸ“‹ Executing diff_tables with options: {diff_options}")
            self.logger.info(f"ğŸš€ Starting actual table comparison (this will execute SQL queries)...")

            # æ‰§è¡Œæ¯”å¯¹
            diff_result = diff_tables(
                source_table_segment,
                target_table_segment,
                **diff_options
            )

            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            self.logger.info(f"âœ… Data-diff job {job_id} completed in {execution_time}s")
            self.logger.info(f"ğŸ“Š SQL execution finished, processing results...")

            # å¤„ç†ç»“æœ
            return self._process_datadiff_result(diff_result, config, job_id, start_time, end_time)

        except Exception as e:
            self.logger.error(f"Data-diff execution for job {job_id} failed: {e}", exc_info=True)
            raise Exception(f"Data-diff execution failed: {str(e)}")

    def _process_datadiff_result(
        self,
        diff_result: Any,
        config: Dict[str, Any],
        job_id: str,
        start_time: datetime,
        end_time: datetime
    ) -> Dict[str, Any]:
        """
        å¤„ç† data-diff ç»“æœ
        """
        try:
            execution_time = (end_time - start_time).total_seconds()

            # è·å–æºè¡¨å’Œç›®æ ‡è¡¨çš„è¡Œæ•°ä¿¡æ¯
            total_rows_source = 0
            total_rows_target = 0

            try:
                # æ­£ç¡®çš„æ–¹æ³•ï¼šä½¿ç”¨ DiffResultWrapper çš„ _get_stats æ–¹æ³•
                if hasattr(diff_result, '_get_stats'):
                    stats = diff_result._get_stats()
                    total_rows_source = stats.table1_count
                    total_rows_target = stats.table2_count
                    self.logger.info(f"ğŸ“Š ä» DiffResultWrapper è·å–è¡Œæ•°: æºè¡¨={total_rows_source}, ç›®æ ‡è¡¨={total_rows_target}")
                elif hasattr(diff_result, 'info_tree') and hasattr(diff_result.info_tree, 'info'):
                    # å¤‡é€‰æ–¹æ³•ï¼šç›´æ¥ä» info_tree è·å–
                    rowcounts = diff_result.info_tree.info.rowcounts
                    if rowcounts:
                        total_rows_source = rowcounts.get(1, 0)
                        total_rows_target = rowcounts.get(2, 0)
                        self.logger.info(f"ğŸ“Š ä» info_tree è·å–è¡Œæ•°: æºè¡¨={total_rows_source}, ç›®æ ‡è¡¨={total_rows_target}")
                else:
                    self.logger.warning("Unable to find row count information in diff_result")
            except Exception as e:
                self.logger.warning(f"Unable to get row counts from diff_result: {e}")

            # è·å–å·®å¼‚æ•°é‡
            total_differences = 0
            try:
                if hasattr(diff_result, '__len__'):
                    total_differences = len(diff_result)
                elif hasattr(diff_result, 'summary') and 'total_differences' in diff_result.summary:
                    total_differences = diff_result.summary['total_differences']
                else:
                    # å°è¯•å°†diff_resultè½¬æ¢ä¸ºåˆ—è¡¨å¹¶è®¡ç®—é•¿åº¦
                    differences_list = list(diff_result)
                    total_differences = len(differences_list)
            except Exception as e:
                self.logger.warning(f"Unable to get total differences from diff_result: {e}")

            # æ„å»ºå·®å¼‚ç»Ÿè®¡ä¿¡æ¯
            differences = {
                "total_differences": total_differences,
                "missing_in_target": 0,
                "missing_in_source": 0,
                "value_differences": 0
            }

            # å°è¯•è·å–æ›´è¯¦ç»†çš„å·®å¼‚ä¿¡æ¯
            try:
                if hasattr(diff_result, 'diff_count_by_type'):
                    diff_counts = diff_result.diff_count_by_type
                    differences["missing_in_target"] = diff_counts.get('missing_in_b', 0)
                    differences["missing_in_source"] = diff_counts.get('missing_in_a', 0)
                    differences["value_differences"] = diff_counts.get('value_different', 0)
            except Exception as e:
                self.logger.warning(f"Unable to get detailed difference counts: {e}")

            # å¦‚æœæ²¡æœ‰æ€»è¡Œæ•°ï¼Œä½¿ç”¨ä¸€ä¸ªé»˜è®¤å€¼ä»¥é¿å…é™¤ä»¥é›¶é”™è¯¯
            if total_rows_source == 0 and total_rows_target == 0:
                rows_compared = max(total_differences, 1) # ç¡®ä¿ä¸ä¼šé™¤ä»¥é›¶
            else:
                rows_compared = min(total_rows_source, total_rows_target)
                if rows_compared == 0:
                    rows_compared = 1  # ç¡®ä¿ä¸ä¼šé™¤ä»¥é›¶

            # è®¡ç®—åŒ¹é…ç‡
            match_rate = 1.0
            if rows_compared > 0:
                match_rate = 1 - (total_differences / rows_compared)

            # è·å–æ ·æœ¬å·®å¼‚
            sample_differences = []
            try:
                # å°è¯•è·å–ä¸€äº›æ ·æœ¬å·®å¼‚
                if hasattr(diff_result, 'diffs'):
                    diffs_sample = list(diff_result.diffs)[:10]  # æœ€å¤šå–10ä¸ªæ ·æœ¬
                    for diff in diffs_sample:
                        diff_type = diff.get('diff_type', 'unknown')
                        key = diff.get('key', {})
                        source_row = diff.get('a_values', None)
                        target_row = diff.get('b_values', None)

                        sample_diff = {
                            "type": diff_type,
                            "key": key
                        }

                        if source_row:
                            sample_diff["source_row"] = source_row

                        if target_row:
                            sample_diff["target_row"] = target_row

                        if diff_type == 'value_different' and source_row and target_row:
                            # æ‰¾å‡ºä¸åŒçš„åˆ—
                            differing_columns = []
                            for col in source_row:
                                if col in target_row and source_row[col] != target_row[col]:
                                    differing_columns.append(col)
                            sample_diff["differing_columns"] = differing_columns

                        sample_differences.append(sample_diff)
            except Exception as e:
                self.logger.warning(f"Unable to get sample differences: {e}")

            # æ„å»ºå®Œæ•´çš„ç»“æœ
            result = {
                "status": "completed",
                "job_id": job_id,
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "execution_time_seconds": execution_time,
                "config": config,
                "statistics": {
                    "source_table": config["source_table"],
                    "target_table": config["target_table"],
                    "total_rows_source": total_rows_source,
                    "total_rows_target": total_rows_target,
                    "rows_compared": rows_compared,
                    "differences": differences,
                    "match_rate": match_rate
                },
                "sample_differences": sample_differences,
                "summary": {
                    "has_differences": total_differences > 0,
                    "match_percentage": round(match_rate * 100, 2),
                    "data_quality_score": "Good" if match_rate > 0.95 else "Fair" if match_rate > 0.8 else "Poor"
                }
            }            # æ·»åŠ ç±»å‹è­¦å‘Šä¿¡æ¯
            type_warnings = config.get("_type_warnings", [])
            ignored_columns_details = config.get("_ignored_columns_details", [])

            if type_warnings:
                # æå–è¢«å¿½ç•¥çš„åˆ—å
                ignored_columns = [col.get('column_name', '') for col in ignored_columns_details]

                # æ£€æŸ¥æ¯”å¯¹é…ç½®ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰å‚ä¸æ¯”å¯¹çš„åˆ—è¢«å¿½ç•¥
                compare_columns = config.get("compare_columns") or config.get("columns_to_compare")
                compare_columns_set = set()
                if compare_columns:
                    if isinstance(compare_columns, str):
                        compare_columns_set = set(col.strip() for col in compare_columns.split(','))
                    elif isinstance(compare_columns, list):
                        compare_columns_set = set(compare_columns)

                # åˆ¤æ–­æ˜¯å¦æœ‰å‚ä¸æ¯”å¯¹çš„åˆ—è¢«å¿½ç•¥
                has_ignored_comparison_columns = False
                if compare_columns_set:
                    # æŒ‡å®šäº†æ¯”å¯¹åˆ—ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰è¢«å¿½ç•¥çš„
                    has_ignored_comparison_columns = any(col.get('column_name') in compare_columns_set for col in ignored_columns_details)
                elif compare_columns is None:
                    # æ²¡æœ‰æŒ‡å®šæ¯”å¯¹åˆ—ï¼ˆæ¯”å¯¹æ‰€æœ‰åˆ—ï¼‰ï¼Œæ‰€ä»¥ä»»ä½•è¢«å¿½ç•¥çš„åˆ—éƒ½å½±å“æ¯”å¯¹
                    has_ignored_comparison_columns = bool(ignored_columns_details)
                else:
                    # compare_columns æ˜¯ç©ºæ•°ç»„ï¼Œè¡¨ç¤ºä¸æ¯”å¯¹ä»»ä½•åˆ—ï¼Œä¸æ”¯æŒçš„åˆ—ä¸å½±å“æ¯”å¯¹
                    has_ignored_comparison_columns = False

                # ç”Ÿæˆç”¨æˆ·å»ºè®®ï¼ˆéœ€è¦ä¼ å…¥source_configå’Œtarget_configï¼Œæš‚æ—¶ä»configè·å–ï¼‰
                user_suggestions = self._generate_user_suggestions(
                    type_warnings,
                    ignored_columns,
                    config.get('_source_config', {}),
                    config.get('_target_config', {})
                )

                result["warnings"] = {
                    "unsupported_types": type_warnings,
                    "message": "ğŸš¨ ä¸¥é‡é”™è¯¯ï¼šæ£€æµ‹åˆ°ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ï¼Œè¿™äº›å­—æ®µè¢«å®Œå…¨å¿½ç•¥ï¼æ¯”å¯¹ç»“æœä¸å¯é ï¼Œå¯èƒ½æ˜¾ç¤º100%åŒ¹é…ä½†å®é™…æœ‰å·®å¼‚ï¼" if has_ignored_comparison_columns else "âš ï¸ æ£€æµ‹åˆ°è¡¨ä¸­åŒ…å«ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ï¼Œä½†è¿™äº›å­—æ®µæœªå‚ä¸æ¯”å¯¹ï¼Œä¸å½±å“æ¯”å¯¹ç»“æœã€‚",
                    "severity": "critical" if has_ignored_comparison_columns else "warning",
                    "impact": "æ¯”å¯¹ç»“æœä¸å¯ä¿¡ï¼Œä¸åº”åŸºäºæ­¤ç»“æœåšå†³ç­–" if has_ignored_comparison_columns else "ä¸å½±å“æ¯”å¯¹ç»“æœï¼Œä»…ä¾›å‚è€ƒ",
                    "recommendation": "1) å¯ç”¨ä¸¥æ ¼ç±»å‹æ£€æŸ¥æ¨¡å¼ç«‹å³å¤±è´¥ 2) é¢„å¤„ç†æ•°æ®è½¬æ¢ç±»å‹ 3) ä»æ¯”å¯¹ä¸­æ’é™¤è¿™äº›å­—æ®µ" if has_ignored_comparison_columns else "å¦‚éœ€æ¯”å¯¹è¿™äº›å­—æ®µï¼Œè¯·å…ˆè¿›è¡Œæ•°æ®ç±»å‹è½¬æ¢",
                    "ignored_columns": ignored_columns_details,  # æ–°å¢ï¼šè¯¦ç»†çš„è¢«å¿½ç•¥å­—æ®µåˆ—è¡¨
                    "user_suggestions": user_suggestions  # æ–°å¢ï¼šç”¨æˆ·å»ºè®®
                }
                # åªæœ‰å½“å®é™…å‚ä¸æ¯”å¯¹çš„å­—æ®µæœ‰é—®é¢˜æ—¶ï¼Œæ‰è®¾ç½®ä¸ºå¤±è´¥
                if has_ignored_comparison_columns:
                    result["summary"]["data_quality_score"] = "Failed"  # æ”¹ä¸º Failed è€Œä¸æ˜¯ Poor
                    result["summary"]["incomplete_comparison"] = True
                    result["summary"]["comparison_invalid"] = True  # æ–°å¢ï¼šæ ‡è®°æ¯”å¯¹æ— æ•ˆ
                    result["statistics"]["warning"] = "âš ï¸ æ¯”å¯¹å¤±è´¥ - å…³é”®å­—æ®µè¢«å¿½ç•¥ï¼Œç»“æœä¸å¯ä¿¡"
                    result["statistics"]["reliability"] = "unreliable"  # æ–°å¢ï¼šå¯é æ€§æ ‡è®°
                else:
                    # è¡¨ä¸­æœ‰ä¸æ”¯æŒçš„å­—æ®µï¼Œä½†æ²¡æœ‰å‚ä¸æ¯”å¯¹ï¼Œæ¯”å¯¹ç»“æœä»ç„¶å¯é 
                    result["summary"]["data_quality_score"] = result["summary"].get("data_quality_score", "Good")
                    result["statistics"]["reliability"] = "reliable"

                result["summary"]["ignored_columns_count"] = len(ignored_columns_details)
                result["summary"]["ignored_columns_list"] = [f"{col['table_name']}.{col['column_name']} ({col['data_type']})" for col in ignored_columns_details]
                result["statistics"]["ignored_columns_details"] = ignored_columns_details  # åœ¨ç»Ÿè®¡ä¸­ä¹ŸåŒ…å«è¯¦ç»†ä¿¡æ¯

            return result

        except Exception as e:
            self.logger.error(f"Failed to process data-diff result: {e}", exc_info=True)
            return {
                "status": "error",
                "job_id": job_id,
                "message": f"Failed to process result: {str(e)}",
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "raw_result": str(diff_result)[:1000] if diff_result else "None"  # æ·»åŠ åŸå§‹ç»“æœçš„æˆªæ–­ç‰ˆæœ¬ä»¥ä¾¿è°ƒè¯•
            }

    async def get_comparison_status(self, job_id: str) -> Dict[str, Any]:
        """
        è·å–æ¯”å¯¹ä»»åŠ¡çŠ¶æ€

        Args:
            job_id: ä»»åŠ¡ID

        Returns:
            ä»»åŠ¡çŠ¶æ€
        """
        if job_id not in self.active_comparisons:
            return {
                "status": "not_found",
                "message": f"Comparison job {job_id} not found"
            }

        job_info = self.active_comparisons[job_id]

        return {
            "job_id": job_id,
            "status": job_info["status"],
            "start_time": job_info["start_time"].isoformat(),
            "end_time": job_info.get("end_time", {}).isoformat() if job_info.get("end_time") else None,
            "config": job_info["config"],
            "error": job_info.get("error")
        }

    def list_active_comparisons(self) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºæ´»åŠ¨çš„æ¯”å¯¹ä»»åŠ¡

        Returns:
            ä»»åŠ¡åˆ—è¡¨
        """
        comparisons = []
        for job_id, job_info in self.active_comparisons.items():
            comparisons.append({
                "job_id": job_id,
                "status": job_info["status"],
                "start_time": job_info["start_time"].isoformat(),
                "source_table": job_info["config"].get("source_table"),
                "target_table": job_info["config"].get("target_table")
            })
        return comparisons

    async def cancel_comparison(self, job_id: str) -> bool:
        """
        å–æ¶ˆæ¯”å¯¹ä»»åŠ¡

        Args:
            job_id: ä»»åŠ¡ID

        Returns:
            æ˜¯å¦æˆåŠŸå–æ¶ˆ
        """
        if job_id not in self.active_comparisons:
            return False

        job_info = self.active_comparisons[job_id]

        if job_info["status"] == "running":
            # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„ä»»åŠ¡å–æ¶ˆé€»è¾‘
            job_info["status"] = "cancelled"
            job_info["end_time"] = datetime.now()
            return True

        return False

    def cleanup_completed_comparisons(self, max_age_hours: int = 24) -> int:
        """
        æ¸…ç†å·²å®Œæˆçš„æ¯”å¯¹ä»»åŠ¡

        Args:
            max_age_hours: æœ€å¤§ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰

        Returns:
            æ¸…ç†çš„ä»»åŠ¡æ•°é‡
        """
        now = datetime.now()
        cleaned_count = 0

        jobs_to_remove = []
        for job_id, job_info in self.active_comparisons.items():
            if job_info["status"] in ["completed", "error", "cancelled"]:
                end_time = job_info.get("end_time", job_info["start_time"])
                age_hours = (now - end_time).total_seconds() / 3600

                if age_hours > max_age_hours:
                    jobs_to_remove.append(job_id)

        for job_id in jobs_to_remove:
            del self.active_comparisons[job_id]
            cleaned_count += 1

        self.logger.info(f"Cleaned up {cleaned_count} old comparison jobs")
        return cleaned_count

    def _generate_user_suggestions(self,
                                  type_warnings: List[str],
                                  ignored_columns: List[str],
                                  source_config: Dict[str, Any],
                                  target_config: Dict[str, Any]) -> List[str]:
        """
        ä¸ºç”¨æˆ·ç”Ÿæˆæœ‰ç”¨çš„å»ºè®®å’Œè§£å†³æ–¹æ¡ˆ

        Args:
            type_warnings: ç±»å‹è­¦å‘Šåˆ—è¡¨
            ignored_columns: è¢«å¿½ç•¥çš„åˆ—
            source_config: æºæ•°æ®åº“é…ç½®
            target_config: ç›®æ ‡æ•°æ®åº“é…ç½®

        Returns:
            å»ºè®®åˆ—è¡¨
        """
        suggestions = []

        source_type = source_config.get('database_type', 'unknown')
        target_type = target_config.get('database_type', 'unknown')

        if type_warnings:
            suggestions.append(f"ğŸ” å‘ç° {len(type_warnings)} ä¸ªç±»å‹å…¼å®¹æ€§é—®é¢˜")

            if ignored_columns:
                suggestions.append(f"ğŸ“‹ ä»¥ä¸‹åˆ—è¢«å¿½ç•¥: {', '.join(ignored_columns)}")

            # é’ˆå¯¹ä¸åŒæ•°æ®åº“ç±»å‹çš„å…·ä½“å»ºè®®
            if source_type == 'postgresql' and target_type == 'clickzetta':
                suggestions.append("ğŸ’¡ PostgreSQL â†’ Clickzetta å»ºè®®:")
                suggestions.append("  â€¢ money ç±»å‹: å»ºè®®è½¬æ¢ä¸º decimal æˆ– numeric")
                suggestions.append("  â€¢ uuid ç±»å‹: å»ºè®®è½¬æ¢ä¸º string æˆ– varchar")
                suggestions.append("  â€¢ inet/macaddr ç±»å‹: å»ºè®®è½¬æ¢ä¸º string")
                suggestions.append("  â€¢ å¯è€ƒè™‘åœ¨ Clickzetta ç«¯åˆ›å»ºè§†å›¾è¿›è¡Œç±»å‹è½¬æ¢")

            suggestions.append("âš™ï¸ è§£å†³æ–¹æ¡ˆ:")
            suggestions.append("  1. å¯ç”¨ä¸¥æ ¼æ¨¡å¼æŸ¥çœ‹è¯¦ç»†ç±»å‹ä¿¡æ¯")
            suggestions.append("  2. åœ¨æ¯”å¯¹å‰é¢„å¤„ç†æ•°æ®ç±»å‹")
            suggestions.append("  3. ä½¿ç”¨ ETL å·¥å…·ç»Ÿä¸€æ•°æ®æ ¼å¼")
            suggestions.append("  4. ä»æ¯”å¯¹é…ç½®ä¸­æ’é™¤ä¸å…¼å®¹çš„åˆ—")

        if not type_warnings and not ignored_columns:
            suggestions.append("âœ… æ‰€æœ‰åˆ—ç±»å‹éƒ½å…¼å®¹ï¼Œå¯ä»¥è¿›è¡Œå®Œæ•´æ¯”å¯¹")

        return suggestions

    async def get_table_schema_preview(self,
                                     connection_config: Dict[str, Any],
                                     table_name: str) -> Dict[str, Any]:
        """
        è·å–è¡¨ç»“æ„é¢„è§ˆï¼Œå¸®åŠ©ç”¨æˆ·äº†è§£è¡¨ç»“æ„

        Args:
            connection_config: æ•°æ®åº“è¿æ¥é…ç½®
            table_name: è¡¨å

        Returns:
            è¡¨ç»“æ„ä¿¡æ¯
        """
        try:
            from data_diff import connect_to_table

            # æ„å»ºè¿æ¥å­—ç¬¦ä¸²
            if connection_config.get('database_type') == 'clickzetta':
                db_info = {
                    "driver": "clickzetta",
                    "username": connection_config.get('username'),
                    "password": connection_config.get('password'),
                    "instance": connection_config.get('instance'),
                    "service": connection_config.get('service'),
                    "workspace": connection_config.get('workspace'),
                    "schema": connection_config.get('db_schema'),
                    "virtualcluster": connection_config.get('vcluster')
                }
            else:
                conn_id = await self.connection_manager.create_connection(connection_config)
                db_info = self.connection_manager._build_connection_string(
                    self.connection_manager.get_connection_config(conn_id)
                )

            # è¿æ¥åˆ°è¡¨å¹¶è·å–schema
            try:
                table_segment = connect_to_table(
                    db_info=db_info,
                    table_name=table_name,
                    key_columns=("id",),  # ä¸´æ—¶ä½¿ç”¨ï¼Œåªæ˜¯ä¸ºäº†è·å–schema
                    thread_count=1
                ).with_schema()

                # éªŒè¯è¿æ¥æ˜¯å¦æ­£å¸¸å·¥ä½œ
                try:
                    # å°è¯•è·å–åŸºæœ¬ä¿¡æ¯æ¥éªŒè¯è¿æ¥
                    _ = table_segment.get_schema()
                    self.logger.info(f"âœ… Schema preview connection verified for table: {table_name}")
                except Exception as schema_test_error:
                    self.logger.error(f"âŒ Schema preview connection failed: {schema_test_error}")
                    raise Exception(f"æ— æ³•è·å–è¡¨ç»“æ„ï¼Œè¿æ¥å¤±è´¥æˆ–è¡¨ä¸å­˜åœ¨: {table_name} - {str(schema_test_error)}")

            except Exception as connect_error:
                self.logger.error(f"âŒ Failed to connect for schema preview: {connect_error}")
                raise Exception(f"æ— æ³•è¿æ¥åˆ°è¡¨ {table_name} è·å–ç»“æ„é¢„è§ˆ: {str(connect_error)}")

            schema_info = {}
            if hasattr(table_segment, '_schema') and table_segment._schema:
                for column, column_type in table_segment._schema.items():
                    schema_info[column] = {
                        'type': str(column_type),
                        'supported': True  # data-diff èƒ½è¯†åˆ«å°±æ˜¯æ”¯æŒçš„
                    }

            # è·å–è¡Œæ•°
            try:
                row_count = table_segment.count()
                schema_info['_row_count'] = row_count
            except Exception as e:
                schema_info['_row_count'] = f"æ— æ³•è·å–: {str(e)}"

            return {
                'status': 'success',
                'table_name': table_name,
                'schema': schema_info,
                'database_type': connection_config.get('database_type')
            }

        except Exception as e:
            return {
                'status': 'error',
                'table_name': table_name,
                'error': str(e),
                'database_type': connection_config.get('database_type')
            }
